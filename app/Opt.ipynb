{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-20T21:42:39.19589Z",
     "iopub.status.busy": "2022-11-20T21:42:39.195291Z",
     "iopub.status.idle": "2022-11-20T21:42:41.191513Z",
     "shell.execute_reply": "2022-11-20T21:42:41.190364Z",
     "shell.execute_reply.started": "2022-11-20T21:42:39.195769Z"
    },
    "id": "wIgQxMc3H0qU"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import xgboost\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv = pd.read_csv('cleaned.csv.gz',\n",
    "                 dtype = {\n",
    "                     'store_nbr' : 'category',\n",
    "                     'family' : 'category',\n",
    "                     'sales': 'float',\n",
    "                     'city': 'category',\n",
    "                     'state': 'category',\n",
    "                     'type': 'category',\n",
    "                     'holiday_type': 'category',\n",
    "                     'holiday_transferred': 'category'\n",
    "                 },\n",
    "                  parse_dates=['date'])\n",
    "all_csv['date'] = pd.to_datetime(all_csv['date']).dt.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all_csv.copy()  # we can start experimenting from here without reloading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for experimentation\n",
    "\n",
    "filter_by_stores = None  # note: please use string here (unlike Mine.ipynb)\n",
    "filter_by_family = None\n",
    "filter_by_dates = None\n",
    "\n",
    "#filter_by_stores = ['1', '2']  # note: please use string here (unlike Mine.ipynb)\n",
    "#filter_by_family = ['DAIRY', 'PRODUCE']\n",
    "#filter_by_family = ['']\n",
    "#filter_by_dates = '2014-06-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_dates == None:\n",
    "    train_start_date = '2013-01-01'\n",
    "else:\n",
    "    train_start_date = filter_by_dates\n",
    "train_end_date = '2017-08-15'\n",
    "test_start_date = '2017-08-16'\n",
    "test_end_date = '2017-08-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_family != None:\n",
    "    all = all[all['family'].isin(filter_by_family)]\n",
    "if filter_by_stores != None:\n",
    "    all = all[all['store_nbr'].isin(filter_by_stores)]\n",
    "if filter_by_dates != None:\n",
    "    all = all[all['date'] >= filter_by_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6816 entries, 8 to 3035139\n",
      "Data columns (total 38 columns):\n",
      " #   Column               Non-Null Count  Dtype    \n",
      "---  ------               --------------  -----    \n",
      " 0   date                 6816 non-null   period[D]\n",
      " 1   store_nbr            6816 non-null   category \n",
      " 2   family               6816 non-null   category \n",
      " 3   sales                6752 non-null   float64  \n",
      " 4   onpromotion          6816 non-null   int64    \n",
      " 5   sales_lag_01         6756 non-null   float64  \n",
      " 6   sales_lag_02         6760 non-null   float64  \n",
      " 7   sales_lag_03         6764 non-null   float64  \n",
      " 8   sales_lag_04         6768 non-null   float64  \n",
      " 9   sales_lag_05         6772 non-null   float64  \n",
      " 10  sales_lag_06         6776 non-null   float64  \n",
      " 11  sales_lag_07         6780 non-null   float64  \n",
      " 12  sales_lag_08         6784 non-null   float64  \n",
      " 13  sales_lag_09         6788 non-null   float64  \n",
      " 14  sales_lag_10         6792 non-null   float64  \n",
      " 15  sales_lag_11         6796 non-null   float64  \n",
      " 16  sales_lag_12         6800 non-null   float64  \n",
      " 17  sales_lag_13         6804 non-null   float64  \n",
      " 18  sales_lag_14         6808 non-null   float64  \n",
      " 19  sales_lag_15         6812 non-null   float64  \n",
      " 20  sales_lag_16         6816 non-null   float64  \n",
      " 21  sales_lag_17         6816 non-null   float64  \n",
      " 22  sales_lag_18         6816 non-null   float64  \n",
      " 23  sales_lag_19         6816 non-null   float64  \n",
      " 24  sales_lag_20         6816 non-null   float64  \n",
      " 25  city                 6816 non-null   category \n",
      " 26  state                6816 non-null   category \n",
      " 27  type                 6816 non-null   category \n",
      " 28  cluster              6816 non-null   int64    \n",
      " 29  month                6816 non-null   int64    \n",
      " 30  day_of_month         6816 non-null   int64    \n",
      " 31  day_of_year          6816 non-null   int64    \n",
      " 32  week_of_year         6816 non-null   int64    \n",
      " 33  day_of_week          6816 non-null   int64    \n",
      " 34  weekday              6816 non-null   int64    \n",
      " 35  year                 6816 non-null   int64    \n",
      " 36  holiday_type         6816 non-null   category \n",
      " 37  holiday_transferred  1028 non-null   category \n",
      "dtypes: category(7), float64(21), int64(9), period[D](1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "all.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    return pd.get_dummies(data=df, columns=['store_nbr', 'family', 'city', 'state', 'type',\n",
    "                                     'cluster', 'holiday_type', 'holiday_transferred', 'weekday'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_ohe = one_hot_encode(all)\n",
    "all_ohe = all_ohe.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))  # remove bad char in column names\n",
    "\n",
    "X = all_ohe[all_ohe['date'] <= train_end_date]\n",
    "X = X.drop(['sales'], axis=1)\n",
    "y = all_ohe[['date', 'sales']][all_ohe['date'] <= train_end_date]\n",
    "y.set_index('date', inplace=True)\n",
    "\n",
    "X_test = all_ohe[all_ohe['date'] >= test_start_date]\n",
    "X_test = X_test.drop(['sales'], axis=1)\n",
    "\n",
    "X.drop('date', axis=1, inplace=True)\n",
    "X_test.drop('date', axis=1, inplace=True)\n",
    "y.set_index(X.index, inplace=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment I -- Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_I = True\n",
    "if run_experiment_I:\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.20928922205788442\n",
      "RMS log-error val:  0.231758418939929\n",
      "RMS log-error train (actual):  0.9087185771821745\n",
      "RMS log-error val (actual):  0.9985451307445373\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_I:\n",
    "    y_pred_train = lr.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = lr.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment II -- Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type' : 'gbdt',  # gradient boosting decision tree\n",
    "    'early_stopping_rounds': 200,\n",
    "    'force_col_wise': True,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 10,\n",
    "    'metric': 'mse',  # mean square error\n",
    "    'num_iterations': 5000,\n",
    "    'num_leaves': 10,\n",
    "    'random_state': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "run_experiment_II = True  # should be true\n",
    "if run_experiment_II:\n",
    "    X_train_lgb = lightgbm.Dataset(data=X_train, label=y_train, feature_name='auto')\n",
    "    X_val_lgb = lightgbm.Dataset(data=X_val, label=y_val, reference=X_train_lgb, feature_name='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 3.08025\tvalid_1's l2: 3.32536\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l2: 2.64088\tvalid_1's l2: 2.8625\n",
      "[3]\ttraining's l2: 2.28494\tvalid_1's l2: 2.49129\n",
      "[4]\ttraining's l2: 1.99155\tvalid_1's l2: 2.18464\n",
      "[5]\ttraining's l2: 1.75269\tvalid_1's l2: 1.93936\n",
      "[6]\ttraining's l2: 1.55293\tvalid_1's l2: 1.73581\n",
      "[7]\ttraining's l2: 1.38871\tvalid_1's l2: 1.56638\n",
      "[8]\ttraining's l2: 1.2528\tvalid_1's l2: 1.43213\n",
      "[9]\ttraining's l2: 1.14397\tvalid_1's l2: 1.32266\n",
      "[10]\ttraining's l2: 1.05567\tvalid_1's l2: 1.23178\n",
      "[11]\ttraining's l2: 0.982369\tvalid_1's l2: 1.16004\n",
      "[12]\ttraining's l2: 0.919926\tvalid_1's l2: 1.10163\n",
      "[13]\ttraining's l2: 0.869092\tvalid_1's l2: 1.05099\n",
      "[14]\ttraining's l2: 0.826757\tvalid_1's l2: 1.01009\n",
      "[15]\ttraining's l2: 0.788786\tvalid_1's l2: 0.980336\n",
      "[16]\ttraining's l2: 0.753583\tvalid_1's l2: 0.945958\n",
      "[17]\ttraining's l2: 0.724031\tvalid_1's l2: 0.917791\n",
      "[18]\ttraining's l2: 0.69783\tvalid_1's l2: 0.900712\n",
      "[19]\ttraining's l2: 0.674961\tvalid_1's l2: 0.88267\n",
      "[20]\ttraining's l2: 0.65527\tvalid_1's l2: 0.867069\n",
      "[21]\ttraining's l2: 0.635683\tvalid_1's l2: 0.852144\n",
      "[22]\ttraining's l2: 0.618378\tvalid_1's l2: 0.84304\n",
      "[23]\ttraining's l2: 0.603779\tvalid_1's l2: 0.83786\n",
      "[24]\ttraining's l2: 0.591505\tvalid_1's l2: 0.829343\n",
      "[25]\ttraining's l2: 0.579092\tvalid_1's l2: 0.823097\n",
      "[26]\ttraining's l2: 0.568474\tvalid_1's l2: 0.815712\n",
      "[27]\ttraining's l2: 0.559804\tvalid_1's l2: 0.811969\n",
      "[28]\ttraining's l2: 0.550477\tvalid_1's l2: 0.808489\n",
      "[29]\ttraining's l2: 0.542975\tvalid_1's l2: 0.809544\n",
      "[30]\ttraining's l2: 0.531802\tvalid_1's l2: 0.806075\n",
      "[31]\ttraining's l2: 0.524114\tvalid_1's l2: 0.805714\n",
      "[32]\ttraining's l2: 0.516524\tvalid_1's l2: 0.802751\n",
      "[33]\ttraining's l2: 0.509771\tvalid_1's l2: 0.803387\n",
      "[34]\ttraining's l2: 0.50362\tvalid_1's l2: 0.804623\n",
      "[35]\ttraining's l2: 0.49801\tvalid_1's l2: 0.802205\n",
      "[36]\ttraining's l2: 0.492981\tvalid_1's l2: 0.803912\n",
      "[37]\ttraining's l2: 0.489222\tvalid_1's l2: 0.800584\n",
      "[38]\ttraining's l2: 0.48239\tvalid_1's l2: 0.800674\n",
      "[39]\ttraining's l2: 0.476237\tvalid_1's l2: 0.80025\n",
      "[40]\ttraining's l2: 0.470983\tvalid_1's l2: 0.799045\n",
      "[41]\ttraining's l2: 0.467405\tvalid_1's l2: 0.797951\n",
      "[42]\ttraining's l2: 0.461968\tvalid_1's l2: 0.797236\n",
      "[43]\ttraining's l2: 0.45723\tvalid_1's l2: 0.793899\n",
      "[44]\ttraining's l2: 0.454025\tvalid_1's l2: 0.794304\n",
      "[45]\ttraining's l2: 0.45049\tvalid_1's l2: 0.793346\n",
      "[46]\ttraining's l2: 0.44639\tvalid_1's l2: 0.793072\n",
      "[47]\ttraining's l2: 0.442613\tvalid_1's l2: 0.79165\n",
      "[48]\ttraining's l2: 0.438648\tvalid_1's l2: 0.789826\n",
      "[49]\ttraining's l2: 0.435054\tvalid_1's l2: 0.789902\n",
      "[50]\ttraining's l2: 0.430441\tvalid_1's l2: 0.788861\n",
      "[51]\ttraining's l2: 0.425729\tvalid_1's l2: 0.788516\n",
      "[52]\ttraining's l2: 0.423452\tvalid_1's l2: 0.7872\n",
      "[53]\ttraining's l2: 0.420252\tvalid_1's l2: 0.786272\n",
      "[54]\ttraining's l2: 0.417357\tvalid_1's l2: 0.786041\n",
      "[55]\ttraining's l2: 0.41448\tvalid_1's l2: 0.786407\n",
      "[56]\ttraining's l2: 0.411401\tvalid_1's l2: 0.786552\n",
      "[57]\ttraining's l2: 0.408482\tvalid_1's l2: 0.786856\n",
      "[58]\ttraining's l2: 0.405306\tvalid_1's l2: 0.784927\n",
      "[59]\ttraining's l2: 0.402033\tvalid_1's l2: 0.781681\n",
      "[60]\ttraining's l2: 0.39935\tvalid_1's l2: 0.782288\n",
      "[61]\ttraining's l2: 0.396676\tvalid_1's l2: 0.780807\n",
      "[62]\ttraining's l2: 0.394382\tvalid_1's l2: 0.781313\n",
      "[63]\ttraining's l2: 0.390461\tvalid_1's l2: 0.780675\n",
      "[64]\ttraining's l2: 0.388004\tvalid_1's l2: 0.78147\n",
      "[65]\ttraining's l2: 0.385359\tvalid_1's l2: 0.782599\n",
      "[66]\ttraining's l2: 0.382758\tvalid_1's l2: 0.782251\n",
      "[67]\ttraining's l2: 0.379802\tvalid_1's l2: 0.781525\n",
      "[68]\ttraining's l2: 0.37707\tvalid_1's l2: 0.779858\n",
      "[69]\ttraining's l2: 0.372632\tvalid_1's l2: 0.781032\n",
      "[70]\ttraining's l2: 0.370741\tvalid_1's l2: 0.781089\n",
      "[71]\ttraining's l2: 0.368922\tvalid_1's l2: 0.781595\n",
      "[72]\ttraining's l2: 0.365484\tvalid_1's l2: 0.779012\n",
      "[73]\ttraining's l2: 0.363519\tvalid_1's l2: 0.779265\n",
      "[74]\ttraining's l2: 0.360659\tvalid_1's l2: 0.778475\n",
      "[75]\ttraining's l2: 0.357881\tvalid_1's l2: 0.779096\n",
      "[76]\ttraining's l2: 0.355784\tvalid_1's l2: 0.778487\n",
      "[77]\ttraining's l2: 0.352304\tvalid_1's l2: 0.78135\n",
      "[78]\ttraining's l2: 0.348933\tvalid_1's l2: 0.782455\n",
      "[79]\ttraining's l2: 0.345629\tvalid_1's l2: 0.780705\n",
      "[80]\ttraining's l2: 0.343635\tvalid_1's l2: 0.780992\n",
      "[81]\ttraining's l2: 0.341694\tvalid_1's l2: 0.779597\n",
      "[82]\ttraining's l2: 0.339176\tvalid_1's l2: 0.779732\n",
      "[83]\ttraining's l2: 0.336759\tvalid_1's l2: 0.778902\n",
      "[84]\ttraining's l2: 0.334898\tvalid_1's l2: 0.777784\n",
      "[85]\ttraining's l2: 0.331913\tvalid_1's l2: 0.778624\n",
      "[86]\ttraining's l2: 0.329916\tvalid_1's l2: 0.778576\n",
      "[87]\ttraining's l2: 0.326711\tvalid_1's l2: 0.778811\n",
      "[88]\ttraining's l2: 0.325066\tvalid_1's l2: 0.778252\n",
      "[89]\ttraining's l2: 0.32314\tvalid_1's l2: 0.778306\n",
      "[90]\ttraining's l2: 0.321096\tvalid_1's l2: 0.778634\n",
      "[91]\ttraining's l2: 0.31922\tvalid_1's l2: 0.775673\n",
      "[92]\ttraining's l2: 0.31713\tvalid_1's l2: 0.776512\n",
      "[93]\ttraining's l2: 0.315367\tvalid_1's l2: 0.775309\n",
      "[94]\ttraining's l2: 0.31251\tvalid_1's l2: 0.777519\n",
      "[95]\ttraining's l2: 0.310316\tvalid_1's l2: 0.775026\n",
      "[96]\ttraining's l2: 0.308075\tvalid_1's l2: 0.774452\n",
      "[97]\ttraining's l2: 0.306352\tvalid_1's l2: 0.774176\n",
      "[98]\ttraining's l2: 0.304924\tvalid_1's l2: 0.773753\n",
      "[99]\ttraining's l2: 0.302985\tvalid_1's l2: 0.775048\n",
      "[100]\ttraining's l2: 0.301139\tvalid_1's l2: 0.774599\n",
      "[101]\ttraining's l2: 0.299297\tvalid_1's l2: 0.773963\n",
      "[102]\ttraining's l2: 0.296744\tvalid_1's l2: 0.773765\n",
      "[103]\ttraining's l2: 0.295025\tvalid_1's l2: 0.774236\n",
      "[104]\ttraining's l2: 0.292622\tvalid_1's l2: 0.77382\n",
      "[105]\ttraining's l2: 0.29164\tvalid_1's l2: 0.773717\n",
      "[106]\ttraining's l2: 0.289337\tvalid_1's l2: 0.772136\n",
      "[107]\ttraining's l2: 0.287777\tvalid_1's l2: 0.772126\n",
      "[108]\ttraining's l2: 0.285406\tvalid_1's l2: 0.773737\n",
      "[109]\ttraining's l2: 0.28307\tvalid_1's l2: 0.773927\n",
      "[110]\ttraining's l2: 0.281267\tvalid_1's l2: 0.772894\n",
      "[111]\ttraining's l2: 0.279576\tvalid_1's l2: 0.772528\n",
      "[112]\ttraining's l2: 0.27773\tvalid_1's l2: 0.774415\n",
      "[113]\ttraining's l2: 0.275475\tvalid_1's l2: 0.77377\n",
      "[114]\ttraining's l2: 0.273948\tvalid_1's l2: 0.770481\n",
      "[115]\ttraining's l2: 0.272418\tvalid_1's l2: 0.770097\n",
      "[116]\ttraining's l2: 0.271168\tvalid_1's l2: 0.769368\n",
      "[117]\ttraining's l2: 0.269922\tvalid_1's l2: 0.769852\n",
      "[118]\ttraining's l2: 0.267988\tvalid_1's l2: 0.771628\n",
      "[119]\ttraining's l2: 0.266069\tvalid_1's l2: 0.770328\n",
      "[120]\ttraining's l2: 0.26466\tvalid_1's l2: 0.770119\n",
      "[121]\ttraining's l2: 0.263486\tvalid_1's l2: 0.770632\n",
      "[122]\ttraining's l2: 0.261924\tvalid_1's l2: 0.770841\n",
      "[123]\ttraining's l2: 0.260013\tvalid_1's l2: 0.769309\n",
      "[124]\ttraining's l2: 0.258624\tvalid_1's l2: 0.770451\n",
      "[125]\ttraining's l2: 0.257282\tvalid_1's l2: 0.770852\n",
      "[126]\ttraining's l2: 0.255506\tvalid_1's l2: 0.769256\n",
      "[127]\ttraining's l2: 0.254087\tvalid_1's l2: 0.768208\n",
      "[128]\ttraining's l2: 0.252772\tvalid_1's l2: 0.769523\n",
      "[129]\ttraining's l2: 0.25184\tvalid_1's l2: 0.769613\n",
      "[130]\ttraining's l2: 0.2502\tvalid_1's l2: 0.769438\n",
      "[131]\ttraining's l2: 0.249426\tvalid_1's l2: 0.769188\n",
      "[132]\ttraining's l2: 0.247714\tvalid_1's l2: 0.768463\n",
      "[133]\ttraining's l2: 0.246743\tvalid_1's l2: 0.767329\n",
      "[134]\ttraining's l2: 0.244855\tvalid_1's l2: 0.766531\n",
      "[135]\ttraining's l2: 0.243714\tvalid_1's l2: 0.766686\n",
      "[136]\ttraining's l2: 0.242187\tvalid_1's l2: 0.766339\n",
      "[137]\ttraining's l2: 0.240221\tvalid_1's l2: 0.766926\n",
      "[138]\ttraining's l2: 0.239133\tvalid_1's l2: 0.767237\n",
      "[139]\ttraining's l2: 0.238135\tvalid_1's l2: 0.76707\n",
      "[140]\ttraining's l2: 0.237113\tvalid_1's l2: 0.767532\n",
      "[141]\ttraining's l2: 0.23638\tvalid_1's l2: 0.766961\n",
      "[142]\ttraining's l2: 0.234577\tvalid_1's l2: 0.764686\n",
      "[143]\ttraining's l2: 0.233179\tvalid_1's l2: 0.764869\n",
      "[144]\ttraining's l2: 0.231587\tvalid_1's l2: 0.765126\n",
      "[145]\ttraining's l2: 0.230687\tvalid_1's l2: 0.764128\n",
      "[146]\ttraining's l2: 0.229936\tvalid_1's l2: 0.764986\n",
      "[147]\ttraining's l2: 0.229016\tvalid_1's l2: 0.76516\n",
      "[148]\ttraining's l2: 0.227049\tvalid_1's l2: 0.764054\n",
      "[149]\ttraining's l2: 0.225362\tvalid_1's l2: 0.76372\n",
      "[150]\ttraining's l2: 0.223499\tvalid_1's l2: 0.764781\n",
      "[151]\ttraining's l2: 0.222697\tvalid_1's l2: 0.764384\n",
      "[152]\ttraining's l2: 0.221589\tvalid_1's l2: 0.762631\n",
      "[153]\ttraining's l2: 0.219814\tvalid_1's l2: 0.763187\n",
      "[154]\ttraining's l2: 0.219048\tvalid_1's l2: 0.76273\n",
      "[155]\ttraining's l2: 0.218343\tvalid_1's l2: 0.762059\n",
      "[156]\ttraining's l2: 0.216591\tvalid_1's l2: 0.76089\n",
      "[157]\ttraining's l2: 0.215603\tvalid_1's l2: 0.760183\n",
      "[158]\ttraining's l2: 0.214623\tvalid_1's l2: 0.759587\n",
      "[159]\ttraining's l2: 0.213697\tvalid_1's l2: 0.759123\n",
      "[160]\ttraining's l2: 0.212383\tvalid_1's l2: 0.758741\n",
      "[161]\ttraining's l2: 0.211055\tvalid_1's l2: 0.760094\n",
      "[162]\ttraining's l2: 0.209728\tvalid_1's l2: 0.760038\n",
      "[163]\ttraining's l2: 0.208635\tvalid_1's l2: 0.761386\n",
      "[164]\ttraining's l2: 0.207531\tvalid_1's l2: 0.760788\n",
      "[165]\ttraining's l2: 0.205964\tvalid_1's l2: 0.760915\n",
      "[166]\ttraining's l2: 0.204909\tvalid_1's l2: 0.76195\n",
      "[167]\ttraining's l2: 0.203804\tvalid_1's l2: 0.763434\n",
      "[168]\ttraining's l2: 0.203064\tvalid_1's l2: 0.764315\n",
      "[169]\ttraining's l2: 0.201717\tvalid_1's l2: 0.764315\n",
      "[170]\ttraining's l2: 0.200348\tvalid_1's l2: 0.764537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171]\ttraining's l2: 0.199387\tvalid_1's l2: 0.764011\n",
      "[172]\ttraining's l2: 0.197458\tvalid_1's l2: 0.763854\n",
      "[173]\ttraining's l2: 0.1966\tvalid_1's l2: 0.761488\n",
      "[174]\ttraining's l2: 0.195455\tvalid_1's l2: 0.761336\n",
      "[175]\ttraining's l2: 0.194324\tvalid_1's l2: 0.763025\n",
      "[176]\ttraining's l2: 0.193571\tvalid_1's l2: 0.763779\n",
      "[177]\ttraining's l2: 0.192727\tvalid_1's l2: 0.763965\n",
      "[178]\ttraining's l2: 0.191959\tvalid_1's l2: 0.764188\n",
      "[179]\ttraining's l2: 0.191085\tvalid_1's l2: 0.76299\n",
      "[180]\ttraining's l2: 0.190247\tvalid_1's l2: 0.761166\n",
      "[181]\ttraining's l2: 0.189809\tvalid_1's l2: 0.761296\n",
      "[182]\ttraining's l2: 0.188997\tvalid_1's l2: 0.761071\n",
      "[183]\ttraining's l2: 0.18798\tvalid_1's l2: 0.760581\n",
      "[184]\ttraining's l2: 0.186727\tvalid_1's l2: 0.760571\n",
      "[185]\ttraining's l2: 0.185686\tvalid_1's l2: 0.760414\n",
      "[186]\ttraining's l2: 0.185\tvalid_1's l2: 0.760588\n",
      "[187]\ttraining's l2: 0.183842\tvalid_1's l2: 0.76059\n",
      "[188]\ttraining's l2: 0.182729\tvalid_1's l2: 0.760463\n",
      "[189]\ttraining's l2: 0.181973\tvalid_1's l2: 0.761721\n",
      "[190]\ttraining's l2: 0.180765\tvalid_1's l2: 0.761305\n",
      "[191]\ttraining's l2: 0.179835\tvalid_1's l2: 0.761417\n",
      "[192]\ttraining's l2: 0.178643\tvalid_1's l2: 0.76246\n",
      "[193]\ttraining's l2: 0.177072\tvalid_1's l2: 0.761849\n",
      "[194]\ttraining's l2: 0.176546\tvalid_1's l2: 0.761548\n",
      "[195]\ttraining's l2: 0.175919\tvalid_1's l2: 0.761798\n",
      "[196]\ttraining's l2: 0.175428\tvalid_1's l2: 0.762367\n",
      "[197]\ttraining's l2: 0.174479\tvalid_1's l2: 0.763974\n",
      "[198]\ttraining's l2: 0.173377\tvalid_1's l2: 0.765065\n",
      "[199]\ttraining's l2: 0.173004\tvalid_1's l2: 0.765348\n",
      "[200]\ttraining's l2: 0.172406\tvalid_1's l2: 0.765139\n",
      "[201]\ttraining's l2: 0.171533\tvalid_1's l2: 0.764674\n",
      "[202]\ttraining's l2: 0.170407\tvalid_1's l2: 0.764188\n",
      "[203]\ttraining's l2: 0.169763\tvalid_1's l2: 0.764281\n",
      "[204]\ttraining's l2: 0.16911\tvalid_1's l2: 0.764185\n",
      "[205]\ttraining's l2: 0.167933\tvalid_1's l2: 0.76368\n",
      "[206]\ttraining's l2: 0.166927\tvalid_1's l2: 0.764203\n",
      "[207]\ttraining's l2: 0.166338\tvalid_1's l2: 0.763663\n",
      "[208]\ttraining's l2: 0.165577\tvalid_1's l2: 0.762597\n",
      "[209]\ttraining's l2: 0.1645\tvalid_1's l2: 0.762364\n",
      "[210]\ttraining's l2: 0.163777\tvalid_1's l2: 0.761718\n",
      "[211]\ttraining's l2: 0.162976\tvalid_1's l2: 0.761884\n",
      "[212]\ttraining's l2: 0.161958\tvalid_1's l2: 0.761245\n",
      "[213]\ttraining's l2: 0.161525\tvalid_1's l2: 0.760979\n",
      "[214]\ttraining's l2: 0.161015\tvalid_1's l2: 0.760675\n",
      "[215]\ttraining's l2: 0.160122\tvalid_1's l2: 0.760818\n",
      "[216]\ttraining's l2: 0.159756\tvalid_1's l2: 0.760803\n",
      "[217]\ttraining's l2: 0.158862\tvalid_1's l2: 0.759905\n",
      "[218]\ttraining's l2: 0.157649\tvalid_1's l2: 0.759831\n",
      "[219]\ttraining's l2: 0.157089\tvalid_1's l2: 0.758869\n",
      "[220]\ttraining's l2: 0.156291\tvalid_1's l2: 0.758317\n",
      "[221]\ttraining's l2: 0.155643\tvalid_1's l2: 0.758675\n",
      "[222]\ttraining's l2: 0.154987\tvalid_1's l2: 0.758985\n",
      "[223]\ttraining's l2: 0.154062\tvalid_1's l2: 0.760139\n",
      "[224]\ttraining's l2: 0.153685\tvalid_1's l2: 0.760578\n",
      "[225]\ttraining's l2: 0.152957\tvalid_1's l2: 0.761046\n",
      "[226]\ttraining's l2: 0.152197\tvalid_1's l2: 0.762625\n",
      "[227]\ttraining's l2: 0.151491\tvalid_1's l2: 0.763623\n",
      "[228]\ttraining's l2: 0.150259\tvalid_1's l2: 0.76427\n",
      "[229]\ttraining's l2: 0.149791\tvalid_1's l2: 0.764062\n",
      "[230]\ttraining's l2: 0.149091\tvalid_1's l2: 0.764326\n",
      "[231]\ttraining's l2: 0.148194\tvalid_1's l2: 0.766436\n",
      "[232]\ttraining's l2: 0.147192\tvalid_1's l2: 0.765936\n",
      "[233]\ttraining's l2: 0.146607\tvalid_1's l2: 0.766718\n",
      "[234]\ttraining's l2: 0.145756\tvalid_1's l2: 0.767488\n",
      "[235]\ttraining's l2: 0.145486\tvalid_1's l2: 0.767485\n",
      "[236]\ttraining's l2: 0.144558\tvalid_1's l2: 0.766541\n",
      "[237]\ttraining's l2: 0.144129\tvalid_1's l2: 0.766952\n",
      "[238]\ttraining's l2: 0.143511\tvalid_1's l2: 0.767194\n",
      "[239]\ttraining's l2: 0.143304\tvalid_1's l2: 0.767169\n",
      "[240]\ttraining's l2: 0.142452\tvalid_1's l2: 0.767334\n",
      "[241]\ttraining's l2: 0.141792\tvalid_1's l2: 0.767814\n",
      "[242]\ttraining's l2: 0.141199\tvalid_1's l2: 0.768202\n",
      "[243]\ttraining's l2: 0.140944\tvalid_1's l2: 0.768383\n",
      "[244]\ttraining's l2: 0.140306\tvalid_1's l2: 0.767768\n",
      "[245]\ttraining's l2: 0.139636\tvalid_1's l2: 0.767376\n",
      "[246]\ttraining's l2: 0.139\tvalid_1's l2: 0.767323\n",
      "[247]\ttraining's l2: 0.138473\tvalid_1's l2: 0.767146\n",
      "[248]\ttraining's l2: 0.137917\tvalid_1's l2: 0.766782\n",
      "[249]\ttraining's l2: 0.137252\tvalid_1's l2: 0.76637\n",
      "[250]\ttraining's l2: 0.136732\tvalid_1's l2: 0.766786\n",
      "[251]\ttraining's l2: 0.136165\tvalid_1's l2: 0.767289\n",
      "[252]\ttraining's l2: 0.135222\tvalid_1's l2: 0.765969\n",
      "[253]\ttraining's l2: 0.134643\tvalid_1's l2: 0.765295\n",
      "[254]\ttraining's l2: 0.134291\tvalid_1's l2: 0.76558\n",
      "[255]\ttraining's l2: 0.133828\tvalid_1's l2: 0.766023\n",
      "[256]\ttraining's l2: 0.132957\tvalid_1's l2: 0.766044\n",
      "[257]\ttraining's l2: 0.131984\tvalid_1's l2: 0.767109\n",
      "[258]\ttraining's l2: 0.131584\tvalid_1's l2: 0.76655\n",
      "[259]\ttraining's l2: 0.130854\tvalid_1's l2: 0.766534\n",
      "[260]\ttraining's l2: 0.130247\tvalid_1's l2: 0.766226\n",
      "[261]\ttraining's l2: 0.129642\tvalid_1's l2: 0.76632\n",
      "[262]\ttraining's l2: 0.129266\tvalid_1's l2: 0.766237\n",
      "[263]\ttraining's l2: 0.128707\tvalid_1's l2: 0.766554\n",
      "[264]\ttraining's l2: 0.128255\tvalid_1's l2: 0.765719\n",
      "[265]\ttraining's l2: 0.127712\tvalid_1's l2: 0.765418\n",
      "[266]\ttraining's l2: 0.12705\tvalid_1's l2: 0.765284\n",
      "[267]\ttraining's l2: 0.1266\tvalid_1's l2: 0.765661\n",
      "[268]\ttraining's l2: 0.125891\tvalid_1's l2: 0.765082\n",
      "[269]\ttraining's l2: 0.125489\tvalid_1's l2: 0.765615\n",
      "[270]\ttraining's l2: 0.125245\tvalid_1's l2: 0.765612\n",
      "[271]\ttraining's l2: 0.124597\tvalid_1's l2: 0.765315\n",
      "[272]\ttraining's l2: 0.124153\tvalid_1's l2: 0.764818\n",
      "[273]\ttraining's l2: 0.123753\tvalid_1's l2: 0.76539\n",
      "[274]\ttraining's l2: 0.123053\tvalid_1's l2: 0.764795\n",
      "[275]\ttraining's l2: 0.122557\tvalid_1's l2: 0.764124\n",
      "[276]\ttraining's l2: 0.122049\tvalid_1's l2: 0.763735\n",
      "[277]\ttraining's l2: 0.121245\tvalid_1's l2: 0.764504\n",
      "[278]\ttraining's l2: 0.120608\tvalid_1's l2: 0.764158\n",
      "[279]\ttraining's l2: 0.120196\tvalid_1's l2: 0.764314\n",
      "[280]\ttraining's l2: 0.119665\tvalid_1's l2: 0.76419\n",
      "[281]\ttraining's l2: 0.119303\tvalid_1's l2: 0.763971\n",
      "[282]\ttraining's l2: 0.118728\tvalid_1's l2: 0.763938\n",
      "[283]\ttraining's l2: 0.117611\tvalid_1's l2: 0.763742\n",
      "[284]\ttraining's l2: 0.117286\tvalid_1's l2: 0.763956\n",
      "[285]\ttraining's l2: 0.116765\tvalid_1's l2: 0.764687\n",
      "[286]\ttraining's l2: 0.116221\tvalid_1's l2: 0.765407\n",
      "[287]\ttraining's l2: 0.115648\tvalid_1's l2: 0.765974\n",
      "[288]\ttraining's l2: 0.115266\tvalid_1's l2: 0.766112\n",
      "[289]\ttraining's l2: 0.114824\tvalid_1's l2: 0.766069\n",
      "[290]\ttraining's l2: 0.113705\tvalid_1's l2: 0.765227\n",
      "[291]\ttraining's l2: 0.113311\tvalid_1's l2: 0.765285\n",
      "[292]\ttraining's l2: 0.11298\tvalid_1's l2: 0.766089\n",
      "[293]\ttraining's l2: 0.112516\tvalid_1's l2: 0.765138\n",
      "[294]\ttraining's l2: 0.112144\tvalid_1's l2: 0.764329\n",
      "[295]\ttraining's l2: 0.111692\tvalid_1's l2: 0.764335\n",
      "[296]\ttraining's l2: 0.110942\tvalid_1's l2: 0.764481\n",
      "[297]\ttraining's l2: 0.110597\tvalid_1's l2: 0.76497\n",
      "[298]\ttraining's l2: 0.110184\tvalid_1's l2: 0.76561\n",
      "[299]\ttraining's l2: 0.109984\tvalid_1's l2: 0.765772\n",
      "[300]\ttraining's l2: 0.109786\tvalid_1's l2: 0.765774\n",
      "[301]\ttraining's l2: 0.109408\tvalid_1's l2: 0.765695\n",
      "[302]\ttraining's l2: 0.109114\tvalid_1's l2: 0.766037\n",
      "[303]\ttraining's l2: 0.108732\tvalid_1's l2: 0.765692\n",
      "[304]\ttraining's l2: 0.108586\tvalid_1's l2: 0.765718\n",
      "[305]\ttraining's l2: 0.107805\tvalid_1's l2: 0.765765\n",
      "[306]\ttraining's l2: 0.107503\tvalid_1's l2: 0.765921\n",
      "[307]\ttraining's l2: 0.107333\tvalid_1's l2: 0.765663\n",
      "[308]\ttraining's l2: 0.106975\tvalid_1's l2: 0.766616\n",
      "[309]\ttraining's l2: 0.106564\tvalid_1's l2: 0.766636\n",
      "[310]\ttraining's l2: 0.106093\tvalid_1's l2: 0.766701\n",
      "[311]\ttraining's l2: 0.105512\tvalid_1's l2: 0.767665\n",
      "[312]\ttraining's l2: 0.10511\tvalid_1's l2: 0.767153\n",
      "[313]\ttraining's l2: 0.104799\tvalid_1's l2: 0.767618\n",
      "[314]\ttraining's l2: 0.104441\tvalid_1's l2: 0.767494\n",
      "[315]\ttraining's l2: 0.104234\tvalid_1's l2: 0.767032\n",
      "[316]\ttraining's l2: 0.103848\tvalid_1's l2: 0.767268\n",
      "[317]\ttraining's l2: 0.103567\tvalid_1's l2: 0.767473\n",
      "[318]\ttraining's l2: 0.103406\tvalid_1's l2: 0.767456\n",
      "[319]\ttraining's l2: 0.103054\tvalid_1's l2: 0.769254\n",
      "[320]\ttraining's l2: 0.102748\tvalid_1's l2: 0.769554\n",
      "[321]\ttraining's l2: 0.10237\tvalid_1's l2: 0.76867\n",
      "[322]\ttraining's l2: 0.102067\tvalid_1's l2: 0.76854\n",
      "[323]\ttraining's l2: 0.101681\tvalid_1's l2: 0.768418\n",
      "[324]\ttraining's l2: 0.101583\tvalid_1's l2: 0.768341\n",
      "[325]\ttraining's l2: 0.101187\tvalid_1's l2: 0.768727\n",
      "[326]\ttraining's l2: 0.100838\tvalid_1's l2: 0.768622\n",
      "[327]\ttraining's l2: 0.100679\tvalid_1's l2: 0.76856\n",
      "[328]\ttraining's l2: 0.100155\tvalid_1's l2: 0.76935\n",
      "[329]\ttraining's l2: 0.0998025\tvalid_1's l2: 0.768959\n",
      "[330]\ttraining's l2: 0.0995509\tvalid_1's l2: 0.769298\n",
      "[331]\ttraining's l2: 0.0993084\tvalid_1's l2: 0.769823\n",
      "[332]\ttraining's l2: 0.0989394\tvalid_1's l2: 0.769731\n",
      "[333]\ttraining's l2: 0.0986379\tvalid_1's l2: 0.769842\n",
      "[334]\ttraining's l2: 0.0983433\tvalid_1's l2: 0.770304\n",
      "[335]\ttraining's l2: 0.097904\tvalid_1's l2: 0.770316\n",
      "[336]\ttraining's l2: 0.0975948\tvalid_1's l2: 0.770119\n",
      "[337]\ttraining's l2: 0.0972859\tvalid_1's l2: 0.77009\n",
      "[338]\ttraining's l2: 0.0967495\tvalid_1's l2: 0.769817\n",
      "[339]\ttraining's l2: 0.0963065\tvalid_1's l2: 0.769722\n",
      "[340]\ttraining's l2: 0.0959279\tvalid_1's l2: 0.770429\n",
      "[341]\ttraining's l2: 0.0954299\tvalid_1's l2: 0.769985\n",
      "[342]\ttraining's l2: 0.0949888\tvalid_1's l2: 0.770882\n",
      "[343]\ttraining's l2: 0.0942714\tvalid_1's l2: 0.772089\n",
      "[344]\ttraining's l2: 0.0939617\tvalid_1's l2: 0.772404\n",
      "[345]\ttraining's l2: 0.0935529\tvalid_1's l2: 0.772116\n",
      "[346]\ttraining's l2: 0.0932927\tvalid_1's l2: 0.771611\n",
      "[347]\ttraining's l2: 0.0929042\tvalid_1's l2: 0.771644\n",
      "[348]\ttraining's l2: 0.0926785\tvalid_1's l2: 0.77152\n",
      "[349]\ttraining's l2: 0.0923831\tvalid_1's l2: 0.771165\n",
      "[350]\ttraining's l2: 0.0921467\tvalid_1's l2: 0.771242\n",
      "[351]\ttraining's l2: 0.0917373\tvalid_1's l2: 0.771591\n",
      "[352]\ttraining's l2: 0.091619\tvalid_1's l2: 0.771504\n",
      "[353]\ttraining's l2: 0.0912734\tvalid_1's l2: 0.770944\n",
      "[354]\ttraining's l2: 0.0908974\tvalid_1's l2: 0.770279\n",
      "[355]\ttraining's l2: 0.0904048\tvalid_1's l2: 0.769952\n",
      "[356]\ttraining's l2: 0.0903087\tvalid_1's l2: 0.769887\n",
      "[357]\ttraining's l2: 0.0899301\tvalid_1's l2: 0.769771\n",
      "[358]\ttraining's l2: 0.0897197\tvalid_1's l2: 0.769593\n",
      "[359]\ttraining's l2: 0.089497\tvalid_1's l2: 0.769227\n",
      "[360]\ttraining's l2: 0.0891655\tvalid_1's l2: 0.769492\n",
      "[361]\ttraining's l2: 0.0888918\tvalid_1's l2: 0.769312\n",
      "[362]\ttraining's l2: 0.088714\tvalid_1's l2: 0.769203\n",
      "[363]\ttraining's l2: 0.0884941\tvalid_1's l2: 0.769335\n",
      "[364]\ttraining's l2: 0.088029\tvalid_1's l2: 0.769207\n",
      "[365]\ttraining's l2: 0.0878384\tvalid_1's l2: 0.769575\n",
      "[366]\ttraining's l2: 0.0874606\tvalid_1's l2: 0.770039\n",
      "[367]\ttraining's l2: 0.0869458\tvalid_1's l2: 0.771189\n",
      "[368]\ttraining's l2: 0.0866322\tvalid_1's l2: 0.771014\n",
      "[369]\ttraining's l2: 0.0861687\tvalid_1's l2: 0.770193\n",
      "[370]\ttraining's l2: 0.0859409\tvalid_1's l2: 0.770656\n",
      "[371]\ttraining's l2: 0.0855255\tvalid_1's l2: 0.770826\n",
      "[372]\ttraining's l2: 0.0854058\tvalid_1's l2: 0.770992\n",
      "[373]\ttraining's l2: 0.0850415\tvalid_1's l2: 0.770806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[374]\ttraining's l2: 0.0847432\tvalid_1's l2: 0.771207\n",
      "[375]\ttraining's l2: 0.0843536\tvalid_1's l2: 0.77109\n",
      "[376]\ttraining's l2: 0.0840959\tvalid_1's l2: 0.770744\n",
      "[377]\ttraining's l2: 0.0837883\tvalid_1's l2: 0.771273\n",
      "[378]\ttraining's l2: 0.0834566\tvalid_1's l2: 0.771106\n",
      "[379]\ttraining's l2: 0.0833779\tvalid_1's l2: 0.771085\n",
      "[380]\ttraining's l2: 0.0832138\tvalid_1's l2: 0.770778\n",
      "[381]\ttraining's l2: 0.082857\tvalid_1's l2: 0.770971\n",
      "[382]\ttraining's l2: 0.0827012\tvalid_1's l2: 0.771202\n",
      "[383]\ttraining's l2: 0.0824927\tvalid_1's l2: 0.771702\n",
      "[384]\ttraining's l2: 0.0821722\tvalid_1's l2: 0.771245\n",
      "[385]\ttraining's l2: 0.0817519\tvalid_1's l2: 0.770176\n",
      "[386]\ttraining's l2: 0.0814709\tvalid_1's l2: 0.770151\n",
      "[387]\ttraining's l2: 0.0810742\tvalid_1's l2: 0.769944\n",
      "[388]\ttraining's l2: 0.0809308\tvalid_1's l2: 0.769869\n",
      "[389]\ttraining's l2: 0.0807303\tvalid_1's l2: 0.769868\n",
      "[390]\ttraining's l2: 0.0804766\tvalid_1's l2: 0.769616\n",
      "[391]\ttraining's l2: 0.0801384\tvalid_1's l2: 0.768602\n",
      "[392]\ttraining's l2: 0.0797511\tvalid_1's l2: 0.767753\n",
      "[393]\ttraining's l2: 0.0795231\tvalid_1's l2: 0.767923\n",
      "[394]\ttraining's l2: 0.0791314\tvalid_1's l2: 0.767602\n",
      "[395]\ttraining's l2: 0.0789393\tvalid_1's l2: 0.767884\n",
      "[396]\ttraining's l2: 0.0788266\tvalid_1's l2: 0.767729\n",
      "[397]\ttraining's l2: 0.0785831\tvalid_1's l2: 0.767269\n",
      "[398]\ttraining's l2: 0.0784981\tvalid_1's l2: 0.767263\n",
      "[399]\ttraining's l2: 0.078018\tvalid_1's l2: 0.768063\n",
      "[400]\ttraining's l2: 0.0777383\tvalid_1's l2: 0.768337\n",
      "[401]\ttraining's l2: 0.0774521\tvalid_1's l2: 0.768287\n",
      "[402]\ttraining's l2: 0.0773796\tvalid_1's l2: 0.7682\n",
      "[403]\ttraining's l2: 0.0772328\tvalid_1's l2: 0.768002\n",
      "[404]\ttraining's l2: 0.07693\tvalid_1's l2: 0.767189\n",
      "[405]\ttraining's l2: 0.0767822\tvalid_1's l2: 0.767054\n",
      "[406]\ttraining's l2: 0.0765249\tvalid_1's l2: 0.766558\n",
      "[407]\ttraining's l2: 0.0762458\tvalid_1's l2: 0.766917\n",
      "[408]\ttraining's l2: 0.0759852\tvalid_1's l2: 0.766945\n",
      "[409]\ttraining's l2: 0.0756313\tvalid_1's l2: 0.766237\n",
      "[410]\ttraining's l2: 0.0753391\tvalid_1's l2: 0.766972\n",
      "[411]\ttraining's l2: 0.0751486\tvalid_1's l2: 0.767153\n",
      "[412]\ttraining's l2: 0.0746729\tvalid_1's l2: 0.765816\n",
      "[413]\ttraining's l2: 0.0744871\tvalid_1's l2: 0.766059\n",
      "[414]\ttraining's l2: 0.0743109\tvalid_1's l2: 0.766367\n",
      "[415]\ttraining's l2: 0.0740356\tvalid_1's l2: 0.766073\n",
      "[416]\ttraining's l2: 0.0737345\tvalid_1's l2: 0.765587\n",
      "[417]\ttraining's l2: 0.073501\tvalid_1's l2: 0.765321\n",
      "[418]\ttraining's l2: 0.0731574\tvalid_1's l2: 0.766042\n",
      "[419]\ttraining's l2: 0.0727941\tvalid_1's l2: 0.765554\n",
      "[420]\ttraining's l2: 0.0725667\tvalid_1's l2: 0.766071\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's l2: 0.156291\tvalid_1's l2: 0.758317\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_II:\n",
    "    lgb = lightgbm.train(\n",
    "        params=lgb_params, \n",
    "        train_set=X_train_lgb,\n",
    "        valid_sets=[X_train_lgb, X_val_lgb],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.09706488341239258\n",
      "RMS log-error val:  0.2018445806471314\n",
      "RMS log-error train (actual):  0.3953365923801808\n",
      "RMS log-error val (actual):  0.8708139540731853\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_II:\n",
    "    y_pred_train = lgb.predict(X_train, num_iteration=lgb.best_iteration)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = lgb.predict(X_val, num_iteration=lgb.best_iteration)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment III -- Random Forest (Too Slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_III = True\n",
    "if run_experiment_III:\n",
    "    random_forest = RandomForestRegressor(random_state=1)\n",
    "    random_forest.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.07991230493098243\n",
      "RMS log-error val:  0.198402429075905\n",
      "RMS log-error train (actual):  0.29627903576647857\n",
      "RMS log-error val (actual):  0.8535536261784764\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_III:\n",
    "    y_pred_train = random_forest.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = random_forest.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment IV -- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:4.31209\tvalidation_1-rmse:4.31876\n",
      "[1]\tvalidation_0-rmse:3.06096\tvalidation_1-rmse:3.08910\n",
      "[2]\tvalidation_0-rmse:2.19004\tvalidation_1-rmse:2.24533\n",
      "[3]\tvalidation_0-rmse:1.59466\tvalidation_1-rmse:1.68469\n",
      "[4]\tvalidation_0-rmse:1.18722\tvalidation_1-rmse:1.32665\n",
      "[5]\tvalidation_0-rmse:0.91980\tvalidation_1-rmse:1.11358\n",
      "[6]\tvalidation_0-rmse:0.73413\tvalidation_1-rmse:0.98363\n",
      "[7]\tvalidation_0-rmse:0.61259\tvalidation_1-rmse:0.92019\n",
      "[8]\tvalidation_0-rmse:0.53715\tvalidation_1-rmse:0.88344\n",
      "[9]\tvalidation_0-rmse:0.48487\tvalidation_1-rmse:0.86593\n",
      "[10]\tvalidation_0-rmse:0.45575\tvalidation_1-rmse:0.85720\n",
      "[11]\tvalidation_0-rmse:0.42824\tvalidation_1-rmse:0.85934\n",
      "[12]\tvalidation_0-rmse:0.40771\tvalidation_1-rmse:0.85419\n",
      "[13]\tvalidation_0-rmse:0.39713\tvalidation_1-rmse:0.85307\n",
      "[14]\tvalidation_0-rmse:0.37414\tvalidation_1-rmse:0.85326\n",
      "[15]\tvalidation_0-rmse:0.36859\tvalidation_1-rmse:0.85377\n",
      "[16]\tvalidation_0-rmse:0.34950\tvalidation_1-rmse:0.85960\n",
      "[17]\tvalidation_0-rmse:0.34067\tvalidation_1-rmse:0.86049\n",
      "[18]\tvalidation_0-rmse:0.33214\tvalidation_1-rmse:0.86248\n",
      "[19]\tvalidation_0-rmse:0.31965\tvalidation_1-rmse:0.86401\n"
     ]
    }
   ],
   "source": [
    "run_experiment_IV = True\n",
    "if run_experiment_IV:\n",
    "    xgb = xgboost.XGBRegressor(n_estimators=20, early_stopping_rounds=100, learning_rate=0.3)\n",
    "    xgb.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.09657077521238491\n",
      "RMS log-error val:  0.19491349654872678\n",
      "RMS log-error train (actual):  0.3971298372479599\n",
      "RMS log-error val (actual):  0.8530671459927017\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_IV:\n",
    "    y_pred_train = xgb.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = xgb.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiment_IV:\n",
    "    xgb.save_model(\"xgb.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment V -- SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "run_experiment_V = False  # slow\n",
    "if run_experiment_V:\n",
    "    # svr = SVR(C=1.0, epsilon=0.2)\n",
    "    svr = SVR(kernel='rbf')\n",
    "    svr.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiment_V:\n",
    "    y_pred_train = svr.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = svr.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (Moment of Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_predict(model, X_test):\n",
    "    X_test_mod = X_test.copy()\n",
    "    output = np.array([])\n",
    "    start_day, end_day = X_test['day_of_month'].min(), X_test['day_of_month'].max()\n",
    "        # we lost the dates, but we still have day_of_month, which is good enough for our experiment\n",
    "        \n",
    "    for day in range(start_day, end_day + 1):\n",
    "        pred = model.predict(X_test_mod[X_test_mod['day_of_month'] == day])\n",
    "        pred[pred < 0] = 0\n",
    "        print(pred)\n",
    "        output = np.concatenate([output, pred], axis=0)\n",
    "        for future in range(day + 1, end_day + 1):\n",
    "            X_test_mod.loc[X_test_mod[X_test_mod['day_of_month'] == future].index,\n",
    "                           f'sales_lag_{(future - day):02d}'] = pred\n",
    "            # fill out future values now that this sales figure is available\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.796066   8.188501   6.73770381 8.1299742 ]\n",
      "[6.49096638 7.6615989  6.5503878  7.61513576]\n",
      "[6.18127423 7.38670381 6.71012241 7.65339628]\n",
      "[6.39348775 7.74445424 6.72689436 7.83433177]\n",
      "[5.78110772 7.00616237 6.7413249  7.82282064]\n",
      "[6.54792113 7.71567454 6.657167   7.70359622]\n",
      "[6.38460818 7.55797529 6.61716318 7.60029266]\n",
      "[6.678503   8.11966272 6.71568936 8.14399525]\n",
      "[6.53379403 7.61798097 6.56229728 7.60897089]\n",
      "[6.18442467 7.34323001 6.7024197  7.58465175]\n",
      "[6.47405237 7.64956907 6.76925998 7.82708655]\n",
      "[5.71600511 7.04749486 6.7413249  7.8259155 ]\n",
      "[6.56600567 7.80389605 6.68898411 7.79301234]\n",
      "[6.38942367 7.51850691 6.63813854 7.61855276]\n",
      "[6.6569364  8.14914337 6.72041857 8.13660534]\n",
      "[6.566807   7.60225586 6.64780762 7.64615229]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = main_predict(lgb, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_index = 3008016 - 3000888  # we inserted 4 Christmas days, 4 x 54 x 33 = 7128, which is the difference\n",
    "#submission = pd.DataFrame({'id': X_test.index - delta_index, 'sales': np.expm1(y_pred_test)})\n",
    "#submission = pd.DataFrame({'id': X_test.index - delta_index, 'sales': max(y_pred_test, 0)})\n",
    "submission = pd.DataFrame({'id': X_test.index - delta_index, 'sales': np.expm1(y_pred_test)})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
