{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-20T21:42:39.19589Z",
     "iopub.status.busy": "2022-11-20T21:42:39.195291Z",
     "iopub.status.idle": "2022-11-20T21:42:41.191513Z",
     "shell.execute_reply": "2022-11-20T21:42:41.190364Z",
     "shell.execute_reply.started": "2022-11-20T21:42:39.195769Z"
    },
    "id": "wIgQxMc3H0qU"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import xgboost\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from xgboost import XGBRegressor\n",
    "#import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv = pd.read_csv('cleaned.csv.gz',\n",
    "                 dtype = {\n",
    "                     'store_nbr' : 'category',\n",
    "                     'family' : 'category',\n",
    "                     'sales': 'float',\n",
    "                     'city': 'category',\n",
    "                     'state': 'category',\n",
    "                     'type': 'category',\n",
    "                     'holiday_type': 'category',\n",
    "                     'holiday_transferred': 'category'\n",
    "                 },\n",
    "                  parse_dates=['date'])\n",
    "all_csv['date'] = pd.to_datetime(all_csv['date']).dt.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all_csv.copy()  # we can start experimenting from here without reloading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for experimentation\n",
    "\n",
    "filter_by_stores = None  # note: please use string here (unlike Mine.ipynb)\n",
    "filter_by_family = None\n",
    "filter_by_dates = None\n",
    "\n",
    "#filter_by_stores = ['1', '2']  # note: please use string here (unlike Mine.ipynb)\n",
    "#filter_by_family = ['DAIRY', 'PRODUCE']\n",
    "#filter_by_family = ['']\n",
    "#filter_by_dates = '2014-06-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_dates == None:\n",
    "    train_start_date = '2013-01-01'\n",
    "else:\n",
    "    train_start_date = filter_by_dates\n",
    "train_end_date = '2017-08-15'\n",
    "test_start_date = '2017-08-16'\n",
    "test_end_date = '2017-08-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_family != None:\n",
    "    all = all[all['family'].isin(filter_by_family)]\n",
    "if filter_by_stores != None:\n",
    "    all = all[all['store_nbr'].isin(filter_by_stores)]\n",
    "if filter_by_dates != None:\n",
    "    all = all[all['date'] >= filter_by_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3036528 entries, 0 to 3036527\n",
      "Data columns (total 38 columns):\n",
      " #   Column               Dtype    \n",
      "---  ------               -----    \n",
      " 0   date                 period[D]\n",
      " 1   store_nbr            category \n",
      " 2   family               category \n",
      " 3   sales                float64  \n",
      " 4   onpromotion          int64    \n",
      " 5   sales_lag_01         float64  \n",
      " 6   sales_lag_02         float64  \n",
      " 7   sales_lag_03         float64  \n",
      " 8   sales_lag_04         float64  \n",
      " 9   sales_lag_05         float64  \n",
      " 10  sales_lag_06         float64  \n",
      " 11  sales_lag_07         float64  \n",
      " 12  sales_lag_08         float64  \n",
      " 13  sales_lag_09         float64  \n",
      " 14  sales_lag_10         float64  \n",
      " 15  sales_lag_11         float64  \n",
      " 16  sales_lag_12         float64  \n",
      " 17  sales_lag_13         float64  \n",
      " 18  sales_lag_14         float64  \n",
      " 19  sales_lag_15         float64  \n",
      " 20  sales_lag_16         float64  \n",
      " 21  sales_lag_17         float64  \n",
      " 22  sales_lag_18         float64  \n",
      " 23  sales_lag_19         float64  \n",
      " 24  sales_lag_20         float64  \n",
      " 25  city                 category \n",
      " 26  state                category \n",
      " 27  type                 category \n",
      " 28  cluster              int64    \n",
      " 29  month                int64    \n",
      " 30  day_of_month         int64    \n",
      " 31  day_of_year          int64    \n",
      " 32  week_of_year         int64    \n",
      " 33  day_of_week          int64    \n",
      " 34  weekday              int64    \n",
      " 35  year                 int64    \n",
      " 36  holiday_type         category \n",
      " 37  holiday_transferred  category \n",
      "dtypes: category(7), float64(21), int64(9), period[D](1)\n",
      "memory usage: 738.4 MB\n"
     ]
    }
   ],
   "source": [
    "all.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    return pd.get_dummies(data=df, columns=['store_nbr', 'family', 'city', 'state', 'type',\n",
    "                                     'cluster', 'holiday_type', 'holiday_transferred', 'weekday'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_ohe = one_hot_encode(all)\n",
    "all_ohe = all_ohe.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))  # remove bad char in column names\n",
    "\n",
    "X = all_ohe[all_ohe['date'] <= train_end_date]\n",
    "X = X.drop(['sales'], axis=1)\n",
    "y = all_ohe[['date', 'sales']][all_ohe['date'] <= train_end_date]\n",
    "y.set_index('date', inplace=True)\n",
    "\n",
    "X_test = all_ohe[all_ohe['date'] >= test_start_date]\n",
    "X_test = X_test.drop(['sales'], axis=1)\n",
    "\n",
    "X.drop('date', axis=1, inplace=True)\n",
    "X_test.drop('date', axis=1, inplace=True)\n",
    "y.set_index(X.index, inplace=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment I -- Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_I = True\n",
    "if run_experiment_I:\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.20928922205788442\n",
      "RMS log-error val:  0.231758418939929\n",
      "RMS log-error train (actual):  0.9087185771821745\n",
      "RMS log-error val (actual):  0.9985451307445373\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_I:\n",
    "    y_pred_train = lr.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = lr.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment II -- Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type' : 'gbdt',  # gradient boosting decision tree\n",
    "    'early_stopping_rounds': 200,\n",
    "    'force_col_wise': True,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 10,\n",
    "    'metric': 'mse',  # mean square error\n",
    "    'num_iterations': 5000,\n",
    "    'num_leaves': 10,\n",
    "    'random_state': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "run_experiment_II = True  # should be true\n",
    "if run_experiment_II:\n",
    "    X_train_lgb = lightgbm.Dataset(data=X_train, label=y_train, feature_name='auto')\n",
    "    X_val_lgb = lightgbm.Dataset(data=X_val, label=y_val, reference=X_train_lgb, feature_name='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 3.08025\tvalid_1's l2: 3.32536\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l2: 2.64088\tvalid_1's l2: 2.8625\n",
      "[3]\ttraining's l2: 2.28494\tvalid_1's l2: 2.49129\n",
      "[4]\ttraining's l2: 1.99155\tvalid_1's l2: 2.18464\n",
      "[5]\ttraining's l2: 1.75269\tvalid_1's l2: 1.93936\n",
      "[6]\ttraining's l2: 1.55293\tvalid_1's l2: 1.73581\n",
      "[7]\ttraining's l2: 1.38871\tvalid_1's l2: 1.56638\n",
      "[8]\ttraining's l2: 1.2528\tvalid_1's l2: 1.43213\n",
      "[9]\ttraining's l2: 1.14397\tvalid_1's l2: 1.32266\n",
      "[10]\ttraining's l2: 1.05567\tvalid_1's l2: 1.23178\n",
      "[11]\ttraining's l2: 0.982369\tvalid_1's l2: 1.16004\n",
      "[12]\ttraining's l2: 0.919926\tvalid_1's l2: 1.10163\n",
      "[13]\ttraining's l2: 0.869092\tvalid_1's l2: 1.05099\n",
      "[14]\ttraining's l2: 0.826757\tvalid_1's l2: 1.01009\n",
      "[15]\ttraining's l2: 0.788786\tvalid_1's l2: 0.980336\n",
      "[16]\ttraining's l2: 0.753583\tvalid_1's l2: 0.945958\n",
      "[17]\ttraining's l2: 0.724031\tvalid_1's l2: 0.917791\n",
      "[18]\ttraining's l2: 0.69783\tvalid_1's l2: 0.900712\n",
      "[19]\ttraining's l2: 0.674961\tvalid_1's l2: 0.88267\n",
      "[20]\ttraining's l2: 0.65527\tvalid_1's l2: 0.867069\n",
      "[21]\ttraining's l2: 0.635683\tvalid_1's l2: 0.852144\n",
      "[22]\ttraining's l2: 0.618378\tvalid_1's l2: 0.84304\n",
      "[23]\ttraining's l2: 0.603779\tvalid_1's l2: 0.83786\n",
      "[24]\ttraining's l2: 0.591505\tvalid_1's l2: 0.829343\n",
      "[25]\ttraining's l2: 0.579092\tvalid_1's l2: 0.823097\n",
      "[26]\ttraining's l2: 0.568474\tvalid_1's l2: 0.815712\n",
      "[27]\ttraining's l2: 0.559804\tvalid_1's l2: 0.811969\n",
      "[28]\ttraining's l2: 0.550477\tvalid_1's l2: 0.808489\n",
      "[29]\ttraining's l2: 0.542975\tvalid_1's l2: 0.809544\n",
      "[30]\ttraining's l2: 0.531802\tvalid_1's l2: 0.806075\n",
      "[31]\ttraining's l2: 0.524114\tvalid_1's l2: 0.805714\n",
      "[32]\ttraining's l2: 0.516524\tvalid_1's l2: 0.802751\n",
      "[33]\ttraining's l2: 0.509771\tvalid_1's l2: 0.803387\n",
      "[34]\ttraining's l2: 0.50362\tvalid_1's l2: 0.804623\n",
      "[35]\ttraining's l2: 0.49801\tvalid_1's l2: 0.802205\n",
      "[36]\ttraining's l2: 0.492981\tvalid_1's l2: 0.803912\n",
      "[37]\ttraining's l2: 0.489222\tvalid_1's l2: 0.800584\n",
      "[38]\ttraining's l2: 0.48239\tvalid_1's l2: 0.800674\n",
      "[39]\ttraining's l2: 0.476237\tvalid_1's l2: 0.80025\n",
      "[40]\ttraining's l2: 0.470983\tvalid_1's l2: 0.799045\n",
      "[41]\ttraining's l2: 0.467405\tvalid_1's l2: 0.797951\n",
      "[42]\ttraining's l2: 0.461968\tvalid_1's l2: 0.797236\n",
      "[43]\ttraining's l2: 0.45723\tvalid_1's l2: 0.793899\n",
      "[44]\ttraining's l2: 0.454025\tvalid_1's l2: 0.794304\n",
      "[45]\ttraining's l2: 0.45049\tvalid_1's l2: 0.793346\n",
      "[46]\ttraining's l2: 0.44639\tvalid_1's l2: 0.793072\n",
      "[47]\ttraining's l2: 0.442613\tvalid_1's l2: 0.79165\n",
      "[48]\ttraining's l2: 0.438648\tvalid_1's l2: 0.789826\n",
      "[49]\ttraining's l2: 0.435054\tvalid_1's l2: 0.789902\n",
      "[50]\ttraining's l2: 0.430441\tvalid_1's l2: 0.788861\n",
      "[51]\ttraining's l2: 0.425729\tvalid_1's l2: 0.788516\n",
      "[52]\ttraining's l2: 0.423452\tvalid_1's l2: 0.7872\n",
      "[53]\ttraining's l2: 0.420252\tvalid_1's l2: 0.786272\n",
      "[54]\ttraining's l2: 0.417357\tvalid_1's l2: 0.786041\n",
      "[55]\ttraining's l2: 0.41448\tvalid_1's l2: 0.786407\n",
      "[56]\ttraining's l2: 0.411401\tvalid_1's l2: 0.786552\n",
      "[57]\ttraining's l2: 0.408482\tvalid_1's l2: 0.786856\n",
      "[58]\ttraining's l2: 0.405306\tvalid_1's l2: 0.784927\n",
      "[59]\ttraining's l2: 0.402033\tvalid_1's l2: 0.781681\n",
      "[60]\ttraining's l2: 0.39935\tvalid_1's l2: 0.782288\n",
      "[61]\ttraining's l2: 0.396676\tvalid_1's l2: 0.780807\n",
      "[62]\ttraining's l2: 0.394382\tvalid_1's l2: 0.781313\n",
      "[63]\ttraining's l2: 0.390461\tvalid_1's l2: 0.780675\n",
      "[64]\ttraining's l2: 0.388004\tvalid_1's l2: 0.78147\n",
      "[65]\ttraining's l2: 0.385359\tvalid_1's l2: 0.782599\n",
      "[66]\ttraining's l2: 0.382758\tvalid_1's l2: 0.782251\n",
      "[67]\ttraining's l2: 0.379802\tvalid_1's l2: 0.781525\n",
      "[68]\ttraining's l2: 0.37707\tvalid_1's l2: 0.779858\n",
      "[69]\ttraining's l2: 0.372632\tvalid_1's l2: 0.781032\n",
      "[70]\ttraining's l2: 0.370741\tvalid_1's l2: 0.781089\n",
      "[71]\ttraining's l2: 0.368922\tvalid_1's l2: 0.781595\n",
      "[72]\ttraining's l2: 0.365484\tvalid_1's l2: 0.779012\n",
      "[73]\ttraining's l2: 0.363519\tvalid_1's l2: 0.779265\n",
      "[74]\ttraining's l2: 0.360659\tvalid_1's l2: 0.778475\n",
      "[75]\ttraining's l2: 0.357881\tvalid_1's l2: 0.779096\n",
      "[76]\ttraining's l2: 0.355784\tvalid_1's l2: 0.778487\n",
      "[77]\ttraining's l2: 0.352304\tvalid_1's l2: 0.78135\n",
      "[78]\ttraining's l2: 0.348933\tvalid_1's l2: 0.782455\n",
      "[79]\ttraining's l2: 0.345629\tvalid_1's l2: 0.780705\n",
      "[80]\ttraining's l2: 0.343635\tvalid_1's l2: 0.780992\n",
      "[81]\ttraining's l2: 0.341694\tvalid_1's l2: 0.779597\n",
      "[82]\ttraining's l2: 0.339176\tvalid_1's l2: 0.779732\n",
      "[83]\ttraining's l2: 0.336759\tvalid_1's l2: 0.778902\n",
      "[84]\ttraining's l2: 0.334898\tvalid_1's l2: 0.777784\n",
      "[85]\ttraining's l2: 0.331913\tvalid_1's l2: 0.778624\n",
      "[86]\ttraining's l2: 0.329916\tvalid_1's l2: 0.778576\n",
      "[87]\ttraining's l2: 0.326711\tvalid_1's l2: 0.778811\n",
      "[88]\ttraining's l2: 0.325066\tvalid_1's l2: 0.778252\n",
      "[89]\ttraining's l2: 0.32314\tvalid_1's l2: 0.778306\n",
      "[90]\ttraining's l2: 0.321096\tvalid_1's l2: 0.778634\n",
      "[91]\ttraining's l2: 0.31922\tvalid_1's l2: 0.775673\n",
      "[92]\ttraining's l2: 0.31713\tvalid_1's l2: 0.776512\n",
      "[93]\ttraining's l2: 0.315367\tvalid_1's l2: 0.775309\n",
      "[94]\ttraining's l2: 0.31251\tvalid_1's l2: 0.777519\n",
      "[95]\ttraining's l2: 0.310316\tvalid_1's l2: 0.775026\n",
      "[96]\ttraining's l2: 0.308075\tvalid_1's l2: 0.774452\n",
      "[97]\ttraining's l2: 0.306352\tvalid_1's l2: 0.774176\n",
      "[98]\ttraining's l2: 0.304924\tvalid_1's l2: 0.773753\n",
      "[99]\ttraining's l2: 0.302985\tvalid_1's l2: 0.775048\n",
      "[100]\ttraining's l2: 0.301139\tvalid_1's l2: 0.774599\n",
      "[101]\ttraining's l2: 0.299297\tvalid_1's l2: 0.773963\n",
      "[102]\ttraining's l2: 0.296744\tvalid_1's l2: 0.773765\n",
      "[103]\ttraining's l2: 0.295025\tvalid_1's l2: 0.774236\n",
      "[104]\ttraining's l2: 0.292622\tvalid_1's l2: 0.77382\n",
      "[105]\ttraining's l2: 0.29164\tvalid_1's l2: 0.773717\n",
      "[106]\ttraining's l2: 0.289337\tvalid_1's l2: 0.772136\n",
      "[107]\ttraining's l2: 0.287777\tvalid_1's l2: 0.772126\n",
      "[108]\ttraining's l2: 0.285406\tvalid_1's l2: 0.773737\n",
      "[109]\ttraining's l2: 0.28307\tvalid_1's l2: 0.773927\n",
      "[110]\ttraining's l2: 0.281267\tvalid_1's l2: 0.772894\n",
      "[111]\ttraining's l2: 0.279576\tvalid_1's l2: 0.772528\n",
      "[112]\ttraining's l2: 0.27773\tvalid_1's l2: 0.774415\n",
      "[113]\ttraining's l2: 0.275475\tvalid_1's l2: 0.77377\n",
      "[114]\ttraining's l2: 0.273948\tvalid_1's l2: 0.770481\n",
      "[115]\ttraining's l2: 0.272418\tvalid_1's l2: 0.770097\n",
      "[116]\ttraining's l2: 0.271168\tvalid_1's l2: 0.769368\n",
      "[117]\ttraining's l2: 0.269922\tvalid_1's l2: 0.769852\n",
      "[118]\ttraining's l2: 0.267988\tvalid_1's l2: 0.771628\n",
      "[119]\ttraining's l2: 0.266069\tvalid_1's l2: 0.770328\n",
      "[120]\ttraining's l2: 0.26466\tvalid_1's l2: 0.770119\n",
      "[121]\ttraining's l2: 0.263486\tvalid_1's l2: 0.770632\n",
      "[122]\ttraining's l2: 0.261924\tvalid_1's l2: 0.770841\n",
      "[123]\ttraining's l2: 0.260013\tvalid_1's l2: 0.769309\n",
      "[124]\ttraining's l2: 0.258624\tvalid_1's l2: 0.770451\n",
      "[125]\ttraining's l2: 0.257282\tvalid_1's l2: 0.770852\n",
      "[126]\ttraining's l2: 0.255506\tvalid_1's l2: 0.769256\n",
      "[127]\ttraining's l2: 0.254087\tvalid_1's l2: 0.768208\n",
      "[128]\ttraining's l2: 0.252772\tvalid_1's l2: 0.769523\n",
      "[129]\ttraining's l2: 0.25184\tvalid_1's l2: 0.769613\n",
      "[130]\ttraining's l2: 0.2502\tvalid_1's l2: 0.769438\n",
      "[131]\ttraining's l2: 0.249426\tvalid_1's l2: 0.769188\n",
      "[132]\ttraining's l2: 0.247714\tvalid_1's l2: 0.768463\n",
      "[133]\ttraining's l2: 0.246743\tvalid_1's l2: 0.767329\n",
      "[134]\ttraining's l2: 0.244855\tvalid_1's l2: 0.766531\n",
      "[135]\ttraining's l2: 0.243714\tvalid_1's l2: 0.766686\n",
      "[136]\ttraining's l2: 0.242187\tvalid_1's l2: 0.766339\n",
      "[137]\ttraining's l2: 0.240221\tvalid_1's l2: 0.766926\n",
      "[138]\ttraining's l2: 0.239133\tvalid_1's l2: 0.767237\n",
      "[139]\ttraining's l2: 0.238135\tvalid_1's l2: 0.76707\n",
      "[140]\ttraining's l2: 0.237113\tvalid_1's l2: 0.767532\n",
      "[141]\ttraining's l2: 0.23638\tvalid_1's l2: 0.766961\n",
      "[142]\ttraining's l2: 0.234577\tvalid_1's l2: 0.764686\n",
      "[143]\ttraining's l2: 0.233179\tvalid_1's l2: 0.764869\n",
      "[144]\ttraining's l2: 0.231587\tvalid_1's l2: 0.765126\n",
      "[145]\ttraining's l2: 0.230687\tvalid_1's l2: 0.764128\n",
      "[146]\ttraining's l2: 0.229936\tvalid_1's l2: 0.764986\n",
      "[147]\ttraining's l2: 0.229016\tvalid_1's l2: 0.76516\n",
      "[148]\ttraining's l2: 0.227049\tvalid_1's l2: 0.764054\n",
      "[149]\ttraining's l2: 0.225362\tvalid_1's l2: 0.76372\n",
      "[150]\ttraining's l2: 0.223499\tvalid_1's l2: 0.764781\n",
      "[151]\ttraining's l2: 0.222697\tvalid_1's l2: 0.764384\n",
      "[152]\ttraining's l2: 0.221589\tvalid_1's l2: 0.762631\n",
      "[153]\ttraining's l2: 0.219814\tvalid_1's l2: 0.763187\n",
      "[154]\ttraining's l2: 0.219048\tvalid_1's l2: 0.76273\n",
      "[155]\ttraining's l2: 0.218343\tvalid_1's l2: 0.762059\n",
      "[156]\ttraining's l2: 0.216591\tvalid_1's l2: 0.76089\n",
      "[157]\ttraining's l2: 0.215603\tvalid_1's l2: 0.760183\n",
      "[158]\ttraining's l2: 0.214623\tvalid_1's l2: 0.759587\n",
      "[159]\ttraining's l2: 0.213697\tvalid_1's l2: 0.759123\n",
      "[160]\ttraining's l2: 0.212383\tvalid_1's l2: 0.758741\n",
      "[161]\ttraining's l2: 0.211055\tvalid_1's l2: 0.760094\n",
      "[162]\ttraining's l2: 0.209728\tvalid_1's l2: 0.760038\n",
      "[163]\ttraining's l2: 0.208635\tvalid_1's l2: 0.761386\n",
      "[164]\ttraining's l2: 0.207531\tvalid_1's l2: 0.760788\n",
      "[165]\ttraining's l2: 0.205964\tvalid_1's l2: 0.760915\n",
      "[166]\ttraining's l2: 0.204909\tvalid_1's l2: 0.76195\n",
      "[167]\ttraining's l2: 0.203804\tvalid_1's l2: 0.763434\n",
      "[168]\ttraining's l2: 0.203064\tvalid_1's l2: 0.764315\n",
      "[169]\ttraining's l2: 0.201717\tvalid_1's l2: 0.764315\n",
      "[170]\ttraining's l2: 0.200348\tvalid_1's l2: 0.764537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171]\ttraining's l2: 0.199387\tvalid_1's l2: 0.764011\n",
      "[172]\ttraining's l2: 0.197458\tvalid_1's l2: 0.763854\n",
      "[173]\ttraining's l2: 0.1966\tvalid_1's l2: 0.761488\n",
      "[174]\ttraining's l2: 0.195455\tvalid_1's l2: 0.761336\n",
      "[175]\ttraining's l2: 0.194324\tvalid_1's l2: 0.763025\n",
      "[176]\ttraining's l2: 0.193571\tvalid_1's l2: 0.763779\n",
      "[177]\ttraining's l2: 0.192727\tvalid_1's l2: 0.763965\n",
      "[178]\ttraining's l2: 0.191959\tvalid_1's l2: 0.764188\n",
      "[179]\ttraining's l2: 0.191085\tvalid_1's l2: 0.76299\n",
      "[180]\ttraining's l2: 0.190247\tvalid_1's l2: 0.761166\n",
      "[181]\ttraining's l2: 0.189809\tvalid_1's l2: 0.761296\n",
      "[182]\ttraining's l2: 0.188997\tvalid_1's l2: 0.761071\n",
      "[183]\ttraining's l2: 0.18798\tvalid_1's l2: 0.760581\n",
      "[184]\ttraining's l2: 0.186727\tvalid_1's l2: 0.760571\n",
      "[185]\ttraining's l2: 0.185686\tvalid_1's l2: 0.760414\n",
      "[186]\ttraining's l2: 0.185\tvalid_1's l2: 0.760588\n",
      "[187]\ttraining's l2: 0.183842\tvalid_1's l2: 0.76059\n",
      "[188]\ttraining's l2: 0.182729\tvalid_1's l2: 0.760463\n",
      "[189]\ttraining's l2: 0.181973\tvalid_1's l2: 0.761721\n",
      "[190]\ttraining's l2: 0.180765\tvalid_1's l2: 0.761305\n",
      "[191]\ttraining's l2: 0.179835\tvalid_1's l2: 0.761417\n",
      "[192]\ttraining's l2: 0.178643\tvalid_1's l2: 0.76246\n",
      "[193]\ttraining's l2: 0.177072\tvalid_1's l2: 0.761849\n",
      "[194]\ttraining's l2: 0.176546\tvalid_1's l2: 0.761548\n",
      "[195]\ttraining's l2: 0.175919\tvalid_1's l2: 0.761798\n",
      "[196]\ttraining's l2: 0.175428\tvalid_1's l2: 0.762367\n",
      "[197]\ttraining's l2: 0.174479\tvalid_1's l2: 0.763974\n",
      "[198]\ttraining's l2: 0.173377\tvalid_1's l2: 0.765065\n",
      "[199]\ttraining's l2: 0.173004\tvalid_1's l2: 0.765348\n",
      "[200]\ttraining's l2: 0.172406\tvalid_1's l2: 0.765139\n",
      "[201]\ttraining's l2: 0.171533\tvalid_1's l2: 0.764674\n",
      "[202]\ttraining's l2: 0.170407\tvalid_1's l2: 0.764188\n",
      "[203]\ttraining's l2: 0.169763\tvalid_1's l2: 0.764281\n",
      "[204]\ttraining's l2: 0.16911\tvalid_1's l2: 0.764185\n",
      "[205]\ttraining's l2: 0.167933\tvalid_1's l2: 0.76368\n",
      "[206]\ttraining's l2: 0.166927\tvalid_1's l2: 0.764203\n",
      "[207]\ttraining's l2: 0.166338\tvalid_1's l2: 0.763663\n",
      "[208]\ttraining's l2: 0.165577\tvalid_1's l2: 0.762597\n",
      "[209]\ttraining's l2: 0.1645\tvalid_1's l2: 0.762364\n",
      "[210]\ttraining's l2: 0.163777\tvalid_1's l2: 0.761718\n",
      "[211]\ttraining's l2: 0.162976\tvalid_1's l2: 0.761884\n",
      "[212]\ttraining's l2: 0.161958\tvalid_1's l2: 0.761245\n",
      "[213]\ttraining's l2: 0.161525\tvalid_1's l2: 0.760979\n",
      "[214]\ttraining's l2: 0.161015\tvalid_1's l2: 0.760675\n",
      "[215]\ttraining's l2: 0.160122\tvalid_1's l2: 0.760818\n",
      "[216]\ttraining's l2: 0.159756\tvalid_1's l2: 0.760803\n",
      "[217]\ttraining's l2: 0.158862\tvalid_1's l2: 0.759905\n",
      "[218]\ttraining's l2: 0.157649\tvalid_1's l2: 0.759831\n",
      "[219]\ttraining's l2: 0.157089\tvalid_1's l2: 0.758869\n",
      "[220]\ttraining's l2: 0.156291\tvalid_1's l2: 0.758317\n",
      "[221]\ttraining's l2: 0.155643\tvalid_1's l2: 0.758675\n",
      "[222]\ttraining's l2: 0.154987\tvalid_1's l2: 0.758985\n",
      "[223]\ttraining's l2: 0.154062\tvalid_1's l2: 0.760139\n",
      "[224]\ttraining's l2: 0.153685\tvalid_1's l2: 0.760578\n",
      "[225]\ttraining's l2: 0.152957\tvalid_1's l2: 0.761046\n",
      "[226]\ttraining's l2: 0.152197\tvalid_1's l2: 0.762625\n",
      "[227]\ttraining's l2: 0.151491\tvalid_1's l2: 0.763623\n",
      "[228]\ttraining's l2: 0.150259\tvalid_1's l2: 0.76427\n",
      "[229]\ttraining's l2: 0.149791\tvalid_1's l2: 0.764062\n",
      "[230]\ttraining's l2: 0.149091\tvalid_1's l2: 0.764326\n",
      "[231]\ttraining's l2: 0.148194\tvalid_1's l2: 0.766436\n",
      "[232]\ttraining's l2: 0.147192\tvalid_1's l2: 0.765936\n",
      "[233]\ttraining's l2: 0.146607\tvalid_1's l2: 0.766718\n",
      "[234]\ttraining's l2: 0.145756\tvalid_1's l2: 0.767488\n",
      "[235]\ttraining's l2: 0.145486\tvalid_1's l2: 0.767485\n",
      "[236]\ttraining's l2: 0.144558\tvalid_1's l2: 0.766541\n",
      "[237]\ttraining's l2: 0.144129\tvalid_1's l2: 0.766952\n",
      "[238]\ttraining's l2: 0.143511\tvalid_1's l2: 0.767194\n",
      "[239]\ttraining's l2: 0.143304\tvalid_1's l2: 0.767169\n",
      "[240]\ttraining's l2: 0.142452\tvalid_1's l2: 0.767334\n",
      "[241]\ttraining's l2: 0.141792\tvalid_1's l2: 0.767814\n",
      "[242]\ttraining's l2: 0.141199\tvalid_1's l2: 0.768202\n",
      "[243]\ttraining's l2: 0.140944\tvalid_1's l2: 0.768383\n",
      "[244]\ttraining's l2: 0.140306\tvalid_1's l2: 0.767768\n",
      "[245]\ttraining's l2: 0.139636\tvalid_1's l2: 0.767376\n",
      "[246]\ttraining's l2: 0.139\tvalid_1's l2: 0.767323\n",
      "[247]\ttraining's l2: 0.138473\tvalid_1's l2: 0.767146\n",
      "[248]\ttraining's l2: 0.137917\tvalid_1's l2: 0.766782\n",
      "[249]\ttraining's l2: 0.137252\tvalid_1's l2: 0.76637\n",
      "[250]\ttraining's l2: 0.136732\tvalid_1's l2: 0.766786\n",
      "[251]\ttraining's l2: 0.136165\tvalid_1's l2: 0.767289\n",
      "[252]\ttraining's l2: 0.135222\tvalid_1's l2: 0.765969\n",
      "[253]\ttraining's l2: 0.134643\tvalid_1's l2: 0.765295\n",
      "[254]\ttraining's l2: 0.134291\tvalid_1's l2: 0.76558\n",
      "[255]\ttraining's l2: 0.133828\tvalid_1's l2: 0.766023\n",
      "[256]\ttraining's l2: 0.132957\tvalid_1's l2: 0.766044\n",
      "[257]\ttraining's l2: 0.131984\tvalid_1's l2: 0.767109\n",
      "[258]\ttraining's l2: 0.131584\tvalid_1's l2: 0.76655\n",
      "[259]\ttraining's l2: 0.130854\tvalid_1's l2: 0.766534\n",
      "[260]\ttraining's l2: 0.130247\tvalid_1's l2: 0.766226\n",
      "[261]\ttraining's l2: 0.129642\tvalid_1's l2: 0.76632\n",
      "[262]\ttraining's l2: 0.129266\tvalid_1's l2: 0.766237\n",
      "[263]\ttraining's l2: 0.128707\tvalid_1's l2: 0.766554\n",
      "[264]\ttraining's l2: 0.128255\tvalid_1's l2: 0.765719\n",
      "[265]\ttraining's l2: 0.127712\tvalid_1's l2: 0.765418\n",
      "[266]\ttraining's l2: 0.12705\tvalid_1's l2: 0.765284\n",
      "[267]\ttraining's l2: 0.1266\tvalid_1's l2: 0.765661\n",
      "[268]\ttraining's l2: 0.125891\tvalid_1's l2: 0.765082\n",
      "[269]\ttraining's l2: 0.125489\tvalid_1's l2: 0.765615\n",
      "[270]\ttraining's l2: 0.125245\tvalid_1's l2: 0.765612\n",
      "[271]\ttraining's l2: 0.124597\tvalid_1's l2: 0.765315\n",
      "[272]\ttraining's l2: 0.124153\tvalid_1's l2: 0.764818\n",
      "[273]\ttraining's l2: 0.123753\tvalid_1's l2: 0.76539\n",
      "[274]\ttraining's l2: 0.123053\tvalid_1's l2: 0.764795\n",
      "[275]\ttraining's l2: 0.122557\tvalid_1's l2: 0.764124\n",
      "[276]\ttraining's l2: 0.122049\tvalid_1's l2: 0.763735\n",
      "[277]\ttraining's l2: 0.121245\tvalid_1's l2: 0.764504\n",
      "[278]\ttraining's l2: 0.120608\tvalid_1's l2: 0.764158\n",
      "[279]\ttraining's l2: 0.120196\tvalid_1's l2: 0.764314\n",
      "[280]\ttraining's l2: 0.119665\tvalid_1's l2: 0.76419\n",
      "[281]\ttraining's l2: 0.119303\tvalid_1's l2: 0.763971\n",
      "[282]\ttraining's l2: 0.118728\tvalid_1's l2: 0.763938\n",
      "[283]\ttraining's l2: 0.117611\tvalid_1's l2: 0.763742\n",
      "[284]\ttraining's l2: 0.117286\tvalid_1's l2: 0.763956\n",
      "[285]\ttraining's l2: 0.116765\tvalid_1's l2: 0.764687\n",
      "[286]\ttraining's l2: 0.116221\tvalid_1's l2: 0.765407\n",
      "[287]\ttraining's l2: 0.115648\tvalid_1's l2: 0.765974\n",
      "[288]\ttraining's l2: 0.115266\tvalid_1's l2: 0.766112\n",
      "[289]\ttraining's l2: 0.114824\tvalid_1's l2: 0.766069\n",
      "[290]\ttraining's l2: 0.113705\tvalid_1's l2: 0.765227\n",
      "[291]\ttraining's l2: 0.113311\tvalid_1's l2: 0.765285\n",
      "[292]\ttraining's l2: 0.11298\tvalid_1's l2: 0.766089\n",
      "[293]\ttraining's l2: 0.112516\tvalid_1's l2: 0.765138\n",
      "[294]\ttraining's l2: 0.112144\tvalid_1's l2: 0.764329\n",
      "[295]\ttraining's l2: 0.111692\tvalid_1's l2: 0.764335\n",
      "[296]\ttraining's l2: 0.110942\tvalid_1's l2: 0.764481\n",
      "[297]\ttraining's l2: 0.110597\tvalid_1's l2: 0.76497\n",
      "[298]\ttraining's l2: 0.110184\tvalid_1's l2: 0.76561\n",
      "[299]\ttraining's l2: 0.109984\tvalid_1's l2: 0.765772\n",
      "[300]\ttraining's l2: 0.109786\tvalid_1's l2: 0.765774\n",
      "[301]\ttraining's l2: 0.109408\tvalid_1's l2: 0.765695\n",
      "[302]\ttraining's l2: 0.109114\tvalid_1's l2: 0.766037\n",
      "[303]\ttraining's l2: 0.108732\tvalid_1's l2: 0.765692\n",
      "[304]\ttraining's l2: 0.108586\tvalid_1's l2: 0.765718\n",
      "[305]\ttraining's l2: 0.107805\tvalid_1's l2: 0.765765\n",
      "[306]\ttraining's l2: 0.107503\tvalid_1's l2: 0.765921\n",
      "[307]\ttraining's l2: 0.107333\tvalid_1's l2: 0.765663\n",
      "[308]\ttraining's l2: 0.106975\tvalid_1's l2: 0.766616\n",
      "[309]\ttraining's l2: 0.106564\tvalid_1's l2: 0.766636\n",
      "[310]\ttraining's l2: 0.106093\tvalid_1's l2: 0.766701\n",
      "[311]\ttraining's l2: 0.105512\tvalid_1's l2: 0.767665\n",
      "[312]\ttraining's l2: 0.10511\tvalid_1's l2: 0.767153\n",
      "[313]\ttraining's l2: 0.104799\tvalid_1's l2: 0.767618\n",
      "[314]\ttraining's l2: 0.104441\tvalid_1's l2: 0.767494\n",
      "[315]\ttraining's l2: 0.104234\tvalid_1's l2: 0.767032\n",
      "[316]\ttraining's l2: 0.103848\tvalid_1's l2: 0.767268\n",
      "[317]\ttraining's l2: 0.103567\tvalid_1's l2: 0.767473\n",
      "[318]\ttraining's l2: 0.103406\tvalid_1's l2: 0.767456\n",
      "[319]\ttraining's l2: 0.103054\tvalid_1's l2: 0.769254\n",
      "[320]\ttraining's l2: 0.102748\tvalid_1's l2: 0.769554\n",
      "[321]\ttraining's l2: 0.10237\tvalid_1's l2: 0.76867\n",
      "[322]\ttraining's l2: 0.102067\tvalid_1's l2: 0.76854\n",
      "[323]\ttraining's l2: 0.101681\tvalid_1's l2: 0.768418\n",
      "[324]\ttraining's l2: 0.101583\tvalid_1's l2: 0.768341\n",
      "[325]\ttraining's l2: 0.101187\tvalid_1's l2: 0.768727\n",
      "[326]\ttraining's l2: 0.100838\tvalid_1's l2: 0.768622\n",
      "[327]\ttraining's l2: 0.100679\tvalid_1's l2: 0.76856\n",
      "[328]\ttraining's l2: 0.100155\tvalid_1's l2: 0.76935\n",
      "[329]\ttraining's l2: 0.0998025\tvalid_1's l2: 0.768959\n",
      "[330]\ttraining's l2: 0.0995509\tvalid_1's l2: 0.769298\n",
      "[331]\ttraining's l2: 0.0993084\tvalid_1's l2: 0.769823\n",
      "[332]\ttraining's l2: 0.0989394\tvalid_1's l2: 0.769731\n",
      "[333]\ttraining's l2: 0.0986379\tvalid_1's l2: 0.769842\n",
      "[334]\ttraining's l2: 0.0983433\tvalid_1's l2: 0.770304\n",
      "[335]\ttraining's l2: 0.097904\tvalid_1's l2: 0.770316\n",
      "[336]\ttraining's l2: 0.0975948\tvalid_1's l2: 0.770119\n",
      "[337]\ttraining's l2: 0.0972859\tvalid_1's l2: 0.77009\n",
      "[338]\ttraining's l2: 0.0967495\tvalid_1's l2: 0.769817\n",
      "[339]\ttraining's l2: 0.0963065\tvalid_1's l2: 0.769722\n",
      "[340]\ttraining's l2: 0.0959279\tvalid_1's l2: 0.770429\n",
      "[341]\ttraining's l2: 0.0954299\tvalid_1's l2: 0.769985\n",
      "[342]\ttraining's l2: 0.0949888\tvalid_1's l2: 0.770882\n",
      "[343]\ttraining's l2: 0.0942714\tvalid_1's l2: 0.772089\n",
      "[344]\ttraining's l2: 0.0939617\tvalid_1's l2: 0.772404\n",
      "[345]\ttraining's l2: 0.0935529\tvalid_1's l2: 0.772116\n",
      "[346]\ttraining's l2: 0.0932927\tvalid_1's l2: 0.771611\n",
      "[347]\ttraining's l2: 0.0929042\tvalid_1's l2: 0.771644\n",
      "[348]\ttraining's l2: 0.0926785\tvalid_1's l2: 0.77152\n",
      "[349]\ttraining's l2: 0.0923831\tvalid_1's l2: 0.771165\n",
      "[350]\ttraining's l2: 0.0921467\tvalid_1's l2: 0.771242\n",
      "[351]\ttraining's l2: 0.0917373\tvalid_1's l2: 0.771591\n",
      "[352]\ttraining's l2: 0.091619\tvalid_1's l2: 0.771504\n",
      "[353]\ttraining's l2: 0.0912734\tvalid_1's l2: 0.770944\n",
      "[354]\ttraining's l2: 0.0908974\tvalid_1's l2: 0.770279\n",
      "[355]\ttraining's l2: 0.0904048\tvalid_1's l2: 0.769952\n",
      "[356]\ttraining's l2: 0.0903087\tvalid_1's l2: 0.769887\n",
      "[357]\ttraining's l2: 0.0899301\tvalid_1's l2: 0.769771\n",
      "[358]\ttraining's l2: 0.0897197\tvalid_1's l2: 0.769593\n",
      "[359]\ttraining's l2: 0.089497\tvalid_1's l2: 0.769227\n",
      "[360]\ttraining's l2: 0.0891655\tvalid_1's l2: 0.769492\n",
      "[361]\ttraining's l2: 0.0888918\tvalid_1's l2: 0.769312\n",
      "[362]\ttraining's l2: 0.088714\tvalid_1's l2: 0.769203\n",
      "[363]\ttraining's l2: 0.0884941\tvalid_1's l2: 0.769335\n",
      "[364]\ttraining's l2: 0.088029\tvalid_1's l2: 0.769207\n",
      "[365]\ttraining's l2: 0.0878384\tvalid_1's l2: 0.769575\n",
      "[366]\ttraining's l2: 0.0874606\tvalid_1's l2: 0.770039\n",
      "[367]\ttraining's l2: 0.0869458\tvalid_1's l2: 0.771189\n",
      "[368]\ttraining's l2: 0.0866322\tvalid_1's l2: 0.771014\n",
      "[369]\ttraining's l2: 0.0861687\tvalid_1's l2: 0.770193\n",
      "[370]\ttraining's l2: 0.0859409\tvalid_1's l2: 0.770656\n",
      "[371]\ttraining's l2: 0.0855255\tvalid_1's l2: 0.770826\n",
      "[372]\ttraining's l2: 0.0854058\tvalid_1's l2: 0.770992\n",
      "[373]\ttraining's l2: 0.0850415\tvalid_1's l2: 0.770806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[374]\ttraining's l2: 0.0847432\tvalid_1's l2: 0.771207\n",
      "[375]\ttraining's l2: 0.0843536\tvalid_1's l2: 0.77109\n",
      "[376]\ttraining's l2: 0.0840959\tvalid_1's l2: 0.770744\n",
      "[377]\ttraining's l2: 0.0837883\tvalid_1's l2: 0.771273\n",
      "[378]\ttraining's l2: 0.0834566\tvalid_1's l2: 0.771106\n",
      "[379]\ttraining's l2: 0.0833779\tvalid_1's l2: 0.771085\n",
      "[380]\ttraining's l2: 0.0832138\tvalid_1's l2: 0.770778\n",
      "[381]\ttraining's l2: 0.082857\tvalid_1's l2: 0.770971\n",
      "[382]\ttraining's l2: 0.0827012\tvalid_1's l2: 0.771202\n",
      "[383]\ttraining's l2: 0.0824927\tvalid_1's l2: 0.771702\n",
      "[384]\ttraining's l2: 0.0821722\tvalid_1's l2: 0.771245\n",
      "[385]\ttraining's l2: 0.0817519\tvalid_1's l2: 0.770176\n",
      "[386]\ttraining's l2: 0.0814709\tvalid_1's l2: 0.770151\n",
      "[387]\ttraining's l2: 0.0810742\tvalid_1's l2: 0.769944\n",
      "[388]\ttraining's l2: 0.0809308\tvalid_1's l2: 0.769869\n",
      "[389]\ttraining's l2: 0.0807303\tvalid_1's l2: 0.769868\n",
      "[390]\ttraining's l2: 0.0804766\tvalid_1's l2: 0.769616\n",
      "[391]\ttraining's l2: 0.0801384\tvalid_1's l2: 0.768602\n",
      "[392]\ttraining's l2: 0.0797511\tvalid_1's l2: 0.767753\n",
      "[393]\ttraining's l2: 0.0795231\tvalid_1's l2: 0.767923\n",
      "[394]\ttraining's l2: 0.0791314\tvalid_1's l2: 0.767602\n",
      "[395]\ttraining's l2: 0.0789393\tvalid_1's l2: 0.767884\n",
      "[396]\ttraining's l2: 0.0788266\tvalid_1's l2: 0.767729\n",
      "[397]\ttraining's l2: 0.0785831\tvalid_1's l2: 0.767269\n",
      "[398]\ttraining's l2: 0.0784981\tvalid_1's l2: 0.767263\n",
      "[399]\ttraining's l2: 0.078018\tvalid_1's l2: 0.768063\n",
      "[400]\ttraining's l2: 0.0777383\tvalid_1's l2: 0.768337\n",
      "[401]\ttraining's l2: 0.0774521\tvalid_1's l2: 0.768287\n",
      "[402]\ttraining's l2: 0.0773796\tvalid_1's l2: 0.7682\n",
      "[403]\ttraining's l2: 0.0772328\tvalid_1's l2: 0.768002\n",
      "[404]\ttraining's l2: 0.07693\tvalid_1's l2: 0.767189\n",
      "[405]\ttraining's l2: 0.0767822\tvalid_1's l2: 0.767054\n",
      "[406]\ttraining's l2: 0.0765249\tvalid_1's l2: 0.766558\n",
      "[407]\ttraining's l2: 0.0762458\tvalid_1's l2: 0.766917\n",
      "[408]\ttraining's l2: 0.0759852\tvalid_1's l2: 0.766945\n",
      "[409]\ttraining's l2: 0.0756313\tvalid_1's l2: 0.766237\n",
      "[410]\ttraining's l2: 0.0753391\tvalid_1's l2: 0.766972\n",
      "[411]\ttraining's l2: 0.0751486\tvalid_1's l2: 0.767153\n",
      "[412]\ttraining's l2: 0.0746729\tvalid_1's l2: 0.765816\n",
      "[413]\ttraining's l2: 0.0744871\tvalid_1's l2: 0.766059\n",
      "[414]\ttraining's l2: 0.0743109\tvalid_1's l2: 0.766367\n",
      "[415]\ttraining's l2: 0.0740356\tvalid_1's l2: 0.766073\n",
      "[416]\ttraining's l2: 0.0737345\tvalid_1's l2: 0.765587\n",
      "[417]\ttraining's l2: 0.073501\tvalid_1's l2: 0.765321\n",
      "[418]\ttraining's l2: 0.0731574\tvalid_1's l2: 0.766042\n",
      "[419]\ttraining's l2: 0.0727941\tvalid_1's l2: 0.765554\n",
      "[420]\ttraining's l2: 0.0725667\tvalid_1's l2: 0.766071\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's l2: 0.156291\tvalid_1's l2: 0.758317\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_II:\n",
    "    lgb = lightgbm.train(\n",
    "        params=lgb_params, \n",
    "        train_set=X_train_lgb,\n",
    "        valid_sets=[X_train_lgb, X_val_lgb],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.09706488341239258\n",
      "RMS log-error val:  0.2018445806471314\n",
      "RMS log-error train (actual):  0.3953365923801808\n",
      "RMS log-error val (actual):  0.8708139540731853\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_II:\n",
    "    y_pred_train = lgb.predict(X_train, num_iteration=lgb.best_iteration)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = lgb.predict(X_val, num_iteration=lgb.best_iteration)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment III -- Random Forest (Too Slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_experiment_III:\n\u001b[1;32m      3\u001b[0m     random_forest \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrandom_forest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment_III = True\n",
    "if run_experiment_III:\n",
    "    random_forest = RandomForestRegressor(random_state=1)\n",
    "    random_forest.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.07991230493098243\n",
      "RMS log-error val:  0.198402429075905\n",
      "RMS log-error train (actual):  0.29627903576647857\n",
      "RMS log-error val (actual):  0.8535536261784764\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_III:\n",
    "    y_pred_train = random_forest.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = random_forest.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment IV -- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "      \"max_depth\": [6,10],\n",
    "      \"n_estimators\": [80,100],\n",
    "#        \"early_stopping_rounds\"=[50,55,60],\n",
    "      \"learning_rate\": [0.01,0.1],\n",
    "      #'colsample_bylevel': [0.1,0.5,1]\n",
    "      \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_IV = True\n",
    "if run_experiment_IV:\n",
    "    xgb = xgboost.XGBRegressor()\n",
    "# xgb.fit(X_train, y_train,\n",
    "#         eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "#         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    n_jobs = -1,\n",
    "    cv = 2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[0]\tvalidation_0-rmse:3.81863\tvalidation_1-rmse:3.82278\n",
      "[0]\tvalidation_0-rmse:3.81863\tvalidation_1-rmse:3.82278\n",
      "[0]\tvalidation_0-rmse:3.81863\tvalidation_1-rmse:3.82278\n",
      "[0]\tvalidation_0-rmse:3.81863\tvalidation_1-rmse:3.82278\n",
      "[0]\tvalidation_0-rmse:3.81854\tvalidation_1-rmse:3.82270\n",
      "[0]\tvalidation_0-rmse:3.81855\tvalidation_1-rmse:3.82270\n",
      "[0]\tvalidation_0-rmse:3.81855\tvalidation_1-rmse:3.82270\n",
      "[0]\tvalidation_0-rmse:3.81854\tvalidation_1-rmse:3.82270\n",
      "[1]\tvalidation_0-rmse:3.78101\tvalidation_1-rmse:3.78513\n",
      "[1]\tvalidation_0-rmse:3.78101\tvalidation_1-rmse:3.78513\n",
      "[1]\tvalidation_0-rmse:3.78101\tvalidation_1-rmse:3.78513\n",
      "[1]\tvalidation_0-rmse:3.78101\tvalidation_1-rmse:3.78513\n",
      "[1]\tvalidation_0-rmse:3.78083\tvalidation_1-rmse:3.78497\n",
      "[1]\tvalidation_0-rmse:3.78085\tvalidation_1-rmse:3.78498\n",
      "[1]\tvalidation_0-rmse:3.78085\tvalidation_1-rmse:3.78498\n",
      "[1]\tvalidation_0-rmse:3.78083\tvalidation_1-rmse:3.78497\n",
      "[2]\tvalidation_0-rmse:3.74378\tvalidation_1-rmse:3.74786\n",
      "[2]\tvalidation_0-rmse:3.74377\tvalidation_1-rmse:3.74785\n",
      "[2]\tvalidation_0-rmse:3.74378\tvalidation_1-rmse:3.74786\n",
      "[2]\tvalidation_0-rmse:3.74351\tvalidation_1-rmse:3.74761\n",
      "[3]\tvalidation_0-rmse:3.70692\tvalidation_1-rmse:3.71097\n",
      "[3]\tvalidation_0-rmse:3.70692\tvalidation_1-rmse:3.71097\n",
      "[3]\tvalidation_0-rmse:3.70690\tvalidation_1-rmse:3.71095\n",
      "[2]\tvalidation_0-rmse:3.74352\tvalidation_1-rmse:3.74763\n",
      "[2]\tvalidation_0-rmse:3.74351\tvalidation_1-rmse:3.74761\n",
      "[2]\tvalidation_0-rmse:3.74352\tvalidation_1-rmse:3.74763\n",
      "[2]\tvalidation_0-rmse:3.74377\tvalidation_1-rmse:3.74785\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS log-error train:  0.09657077521238491\n",
      "RMS log-error val:  0.19491349654872678\n",
      "RMS log-error train (actual):  0.3971298372479599\n",
      "RMS log-error val (actual):  0.8530671459927017\n"
     ]
    }
   ],
   "source": [
    "if run_experiment_IV:\n",
    "    y_pred_train = xgb.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = xgb.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment V -- SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "run_experiment_V = False  # slow\n",
    "if run_experiment_V:\n",
    "    # svr = SVR(C=1.0, epsilon=0.2)\n",
    "    svr = SVR(kernel='rbf')\n",
    "    svr.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiment_V:\n",
    "    y_pred_train = svr.predict(X_train)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    y_pred_val = svr.predict(X_val)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    print(\"RMS log-error train: \", np.sqrt(mean_squared_log_error(y_train, y_pred_train)))\n",
    "    print(\"RMS log-error val: \", np.sqrt(mean_squared_log_error(y_val, y_pred_val)))\n",
    "    print(\"RMS log-error train (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print(\"RMS log-error val (actual): \",\n",
    "          np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(y_pred_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (Moment of Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_predict(model, X_test):\n",
    "    X_test_mod = X_test.copy()\n",
    "    output = np.array([])\n",
    "    start_day, end_day = X_test['day_of_month'].min(), X_test['day_of_month'].max()\n",
    "        # we lost the dates, but we still have day_of_month, which is good enough for our experiment\n",
    "        \n",
    "    for day in range(start_day, end_day + 1):\n",
    "        pred = model.predict(X_test_mod[X_test_mod['day_of_month'] == day])\n",
    "        pred[pred < 0] = 0\n",
    "        print(pred)\n",
    "        output = np.concatenate([output, pred], axis=0)\n",
    "        for future in range(day + 1, end_day + 1):\n",
    "            X_test_mod.loc[X_test_mod[X_test_mod['day_of_month'] == future].index,\n",
    "                           f'sales_lag_{(future - day):02d}'] = pred\n",
    "            # fill out future values now that this sales figure is available\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m main_predict(\u001b[43msvr\u001b[49m, X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svr' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_test = main_predict(svr, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_index = 3008016 - 3000888  # we inserted 4 Christmas days, 4 x 54 x 33 = 7128, which is the difference\n",
    "#submission = pd.DataFrame({'id': X_test.index - delta_index, 'sales': np.expm1(y_pred_test)})\n",
    "#submission = pd.DataFrame({'id': X_test.index - delta_index, 'sales': max(y_pred_test, 0)})\n",
    "submission = pd.DataFrame({'id': X_test.index - delta_index, 'sales': np.expm1(y_pred_test)})\n",
    "submission.to_csv('submissionxgboost.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
